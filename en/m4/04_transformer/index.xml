<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformer models in 2023 on Social / Business Data Science 2023</title><link>https://aaubs.github.io/ds23/en/m4/04_transformer/</link><description>Recent content in Transformer models in 2023 on Social / Business Data Science 2023</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://aaubs.github.io/ds23/en/m4/04_transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>Fine-tuning transformers</title><link>https://aaubs.github.io/ds23/en/m4/04_transformer/1_basictm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m4/04_transformer/1_basictm/</guid><description>Resources You will need a HFü§ó account to upload models We will use WandB for monitoring - get an account if you like to follow that part Notebooks Transformer-based Translation (inference)
Finetuning Sequence Classification with Bert
Homework - As a preparation for final assignment‚ÄºÔ∏è Finetune a model for token-classification Follow this tutorial Cheating option 1: NERDA, Thanks to the lovely people at Ekstrabladet/PIN Cheating option 2: Simpletransformers Try custom Named Entities Skills, Scientific Terms Make your own NER dataset using Argilla and 0-shot labeling with Flair - you can deploy argilla with 1-click on ü§ó spaces and connect to it via e.</description></item><item><title>Timeseries Transformers intro</title><link>https://aaubs.github.io/ds23/en/m4/04_transformer/2_tst_intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m4/04_transformer/2_tst_intro/</guid><description>In this segment, we will review deep learning based approaches to timeseries forecasting. We will focus on timeseries transformers, particularly Google&amp;rsquo;s Temporal Fusion transformers.
Ressources TFT colab TFT Paper</description></item><item><title>Exercise Session 4</title><link>https://aaubs.github.io/ds23/en/m4/04_transformer/3_exercise-session-4_pytorch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m4/04_transformer/3_exercise-session-4_pytorch/</guid><description>In this session, you will learn how to upload your fine-tuned HuggingFace model and deploy it using two different platforms: Streamlit and Gradio. First, you will learn how to properly upload your model to the HuggingFace Hub, a platform that allows you to easily share and access models. Then, you will learn how to use Streamlit and Gradio, two user-friendly platforms, to deploy your model and make it accessible to others.</description></item></channel></rss>