[{"uri":"https://aaubs.github.io/ds23/en/m1/03_uml/01_intro_uml/","title":"- Introduction to Unsupervised ML","tags":[],"description":"","content":"\nThis session will introduce the principles and applications of unsupervised machine learning (UML). Students will learn about the different types of UML problems, and they will explore some of the most popular UML algorithms, such as PCA, SVD, and NMF.\nNotebook(s) Hands-on Intro to Dimensionality reduction and Clustering Recommended Datacamp exercises: Python Recommended Readings and resources Python Data Science Handbook Chapter 5\nWhat Is Machine Learning? Introducing Scikit-Learn Feature Engineering In Depth: Principal Component Analysis In Depth: k-Means Clustering Implementation tutorials on YT PCA and K-means from this list\nIntro slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/01_intro_ds/01_welcome_ds/","title":"- Welcome Students!","tags":[],"description":"","content":"\nThis session will introduce you to the fundamentals of data science, with a focus on Python. We will cover the Python data science stack, essential tools and platforms, software setup, semester overview, and Python 101.\nSession 1: Welcome Students! This session sets the stage for your data science journey:\nPython Data Science Stack: Dive into Python\u0026rsquo;s core data science libraries and frameworks. We\u0026rsquo;ve got you covered!\nEcosystem Deep Dive: Familiarize yourself with essential tools and platforms, such as Github, UCloud, Google Colab, and Jupyter. These will be integral to your studies and projects.\nSoftware Setup: We\u0026rsquo;ll guide you through installing the crucial software. And don\u0026rsquo;t worry, our Teaching Assistants are here to assist with any challenges.\nSemester Overview: Get a glimpse of what the upcoming weeks hold for you.\nPython 101: We\u0026rsquo;ll ensure everyone is up to speed with Python basics.\nNotebooks How to build a development environment using Colab, Google Drive, GitHub, and Kaggle!\n🚀 Notebook: Python 101\n"},{"uri":"https://aaubs.github.io/ds23/en/info/","title":"Info, Schedule &amp; Co","tags":[],"description":"","content":"General info about the semester will be updated here. Check out the calendar on moodle.\nIntro to the semester and module: "},{"uri":"https://aaubs.github.io/ds23/en/m1/02_rapid_prototyping/01_rapid_prototyping/","title":"Introduction to Streamlit: Building an Employee Attrition Dashboard","tags":[],"description":"","content":" Corgi working on a Data Science project. 2023. Roman x Stable Diffusion XL\nStreamlit has rapidly become a go-to tool for data scientists and developers wanting to turn data scripts into shareable web apps. Let\u0026rsquo;s explore its core features and benefits:\nKey Features of Streamlit Simplicity: With just a few lines of Python code, you can have a running web application. No need to deal with HTML, CSS, or JavaScript unless you want to. Interactive Widgets: Streamlit offers out-of-the-box widgets like sliders, buttons, and text inputs that make your app interactive. Data Integration: It seamlessly integrates with popular data science libraries like Pandas, Numpy, Matplotlib, and others. Data Caching: With @st.cache, Streamlit caches the output of functions, ensuring your data operations are efficient and your apps remain performant. Hot Reloading: As you save changes to your script, the app refreshes in real-time. No need to manually restart your app. vs. Gradio While Streamlit offers a robust platform for creating web apps, Gradio provides several distinct advantages, especially when the focus is on deploying machine learning models:\nEase of Model Deployment: Gradio prioritizes simplifying the deployment of machine learning models. Its intuitive Python API allows for quick interface generation, ensuring accessibility even for non-experts. Diverse Input Types: Gradio supports a wide variety of input formats, from images and text to audio, providing flexibility especially when dealing with different data types. Multi-Model Deployment: Gradio stands out with its capability to simultaneously deploy multiple models, perfect for ensemble methods or side-by-side model comparisons. Shareability: Gradio\u0026rsquo;s feature of generating shareable URLs makes collaboration and showcasing a breeze. Security Features: Gradio offers built-in adversarial robustness, adding an extra layer of protection against potential adversarial attacks on deployed models. These unique features make Gradio an attractive option for projects that focus on deploying and sharing machine learning models with diverse requirements. We are going to explore gradio after introductions to ML.\nWhy Choose Streamlit? Rapid Prototyping: Streamlit\u0026rsquo;s intuitive API and hot reloading mean you can quickly iterate and refine your app. Open Source: Being open-source, it boasts a strong community that contributes to its growth and offers a plethora of community plugins. Deployment Ready: With platforms like Streamlit Sharing, deploying your app to the world is just a click away. Extensible: You can integrate advanced JavaScript features or even other Python libraries to extend Streamlit\u0026rsquo;s capabilities. Core Components of Streamlit Layout and Widgets Layout: st.columns and st.container can be used to design your app\u0026rsquo;s layout. Widgets: These are interactive elements like st.slider(), st.selectbox(), and st.button() that capture user input. Display Elements Media: Display images, videos, or audio clips using st.image(), st.video(), and st.audio(). Charts: Use st.line_chart(), st.bar_chart(), or integrate with libraries like Altair for custom visualizations. Tables: Showcase data with st.table() or st.dataframe(). Session State State: Store user data or app state across reruns with st.session_state. Dive Deeper To truly master Streamlit, it\u0026rsquo;s recommended to experiment with building various apps and exploring its official documentation. The community is active, and there\u0026rsquo;s always something new to learn!\nWhat You\u0026rsquo;ll Build By the end of this tutorial, you\u0026rsquo;ll have a functional dashboard that allows users to:\nFilter data based on various employee metrics. Visualize attrition rates and patterns across different dimensions. Understand insights from visualizations. Get actionable recommendations based on data insights. Prerequisites Before you begin, ensure you have the following:\nBasic knowledge of Python. Streamlit installed (pip install streamlit). Familiarity with data visualization libraries like altair, matplotlib, and seaborn. Getting Started 1. Data Source We\u0026rsquo;ll use a synthetic dataset on employee attrition from this link.\n2. File Structure Ensure your working directory has the following structure:\n📦Your_Directory ┣ 📜app.py ┗ 📜requirements.txt ┗ 📜data.csv app.py will contain our Streamlit app\u0026rsquo;s code, while requirements.txt will list the necessary Python packages.\n3. Hosting the App Once you\u0026rsquo;ve built the app, we\u0026rsquo;ll first host it on Streamlit Cloud and then on uCloud (A Danish private cloud for universities).\nBuilding the Dashboard Here is the code\nInitialization We start by importing necessary libraries and loading our dataset. The data is cached using @st.cache_data to enhance performance.\nDesigning the Interface Streamlit provides intuitive functions to design the user interface:\nst.title() and st.header() set titles and headers. st.markdown() allows for rich text formatting. st.sidebar lets you add interactive widgets in a sidebar for filtering. Visualizations Depending on user input, we visualize the filtered data using various charts. For instance, altair is used for bar and pie charts, while matplotlib and seaborn provide KDE plots and boxplots.\nInsights and Recommendations Finally, expanders provide a space to share insights derived from visualizations and actionable recommendations.\nAccess the Deployed App You can access the final deployed Employee Attrition Dashboard on Streamlit using the link below:\nEmployee Attrition Dashboard\nFeel free to explore the app, interact with the various filters and visualizations, and gain insights into employee attrition patterns.\nConclusion Streamlit offers a user-friendly platform to build and deploy interactive data apps without the need for extensive web development skills. Dive in, explore the code, and customize it to create your own data-driven web applications.\nHappy Coding! 🚀\nOld corgy from last year\u0026hellip; Corgi working on a Data Science project. 2022. Roman x Stable Diffusion\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/01_intro_ds/02_data_handeling/","title":"- Data Handeling and Manipulation","tags":[],"description":"","content":"\nThis session will introduce students to the foundational aspects of data handling in Python. Students will learn about the different types of data that are important in data science, and they will explore essential operations like arrange, group-by, filter, select, and join. By the end of this session, students should have a solid understanding of primary data manipulation techniques, setting the stage for more advanced subjects.\nSession 2: Data Handling and Manipulation I (Lecture) In this session, we\u0026rsquo;ll explore the foundational aspects of data handling. Key takeaways include:\nData Manipulation: Learn essential operations like arrange, group-by, filter, select, and join, preparing data for analysis. Notebook: Python Data Manipulation By the end, students should have a solid understanding of primary data manipulation techniques, setting the stage for more advanced subjects.\nExercises Notebook: Python DS Handbook, C.2-3 exercises\nNotebook: Pandas exercises\nNotebook: Netflix Pandas exercises\nNotebook: HR Attrition Assignment\nHere, you will find the answers to the exercises:\nNotebook: Python DS Handbook, C.2-3 exercises and solutions\nNotebook: Pandas exercises and solutions\nNotebook: HR Attrition Assignment solutions\nNotebook: Netflix Pandas exercise solutions\nTutorial - Assignment Examples Notebook: EDA Assignment Example - Policing Further studies Recommended DataCamp courses Introduction to Python Intermediate Python Recommended readings Python fo Data Science Handbook (VanderPlas, 2016), Chapter 2-3 "},{"uri":"https://aaubs.github.io/ds23/en/m1/03_uml/02_recommender_simsea_uml/","title":"- Recommendation and Similarity Search","tags":[],"description":"","content":"\nIn this workshop we are going to learn about recommender systems as a type of UML. Such systems are probably the most widely used and commercialy valuable form of AI today. Specifically we will be looking into collaborative filtering and matrix factorization.\nPlan for today Collaborative filtering / SVD recommender using Nomadlist Trips-data in a Notebooks Streamlit recommender-app Notebook(s) Nomadlist Trips Notebook Recap Nomadlist Trips Notebook Recap Similarity Notebook Recap Dimensionality Reduction App Source code for the app Deployed recommender app Recommended Readings and resources This excellent PyData Talk by the developer of LightFM Recommended Datacamp exercises: Beginner Tutorial RecSys-Course "},{"uri":"https://aaubs.github.io/ds23/en/m1/01_intro_ds/","title":"A) Introduction to Data Science (W35-36)","tags":[],"description":"","content":" Note: Group Portfolio Assignment - Exploratory Data Analysis (EDA) Deadline: Friday, 8 September 2023, 12:00 PM\nThis topic includes 5 sessions as follows:\nWelcome to Data Science! (Friday, September 1st, 10:15-14:15): This session will introduce students to the fundamentals of data science, with a focus on Python. Students will learn about the Python data science stack, essential tools and platforms, and software setup. They will also get a preview of the upcoming weeks and a refresher on Python basics. Data Handling and Manipulation I (Lecture) (Monday, September 4th, 12:30-16:15): This session will cover the foundational aspects of data handling in Python. Students will learn about the different types of data that are important in data science, and they will explore essential operations like arrange, group-by, filter, select, and join. By the end of this session, students should have a solid understanding of primary data manipulation techniques. Exploratory Data Analysis \u0026amp; Essential Statistics (Tuesday, September 5th, 08:15-12:00): This session will introduce students to exploratory data analysis (EDA) and essential statistics. Students will learn how to use EDA to uncover patterns, anomalies, and frame questions in data. They will also learn about foundational measures and techniques for data interpretation. EDA-Exercise on a Real Dataset (Tuesday, September 5th, 12:30-14:15): This session will give students the opportunity to practice the EDA techniques they learned earlier on a real dataset. Students will be able to dive into a real dataset, identify patterns and insights, and implement key EDA methods. They will also benefit from on-the-spot guidance by the teacher and TAs during the exercise. Data Visualization in Data Science (Wednesday, September 6th, 10:15-14:15): This session will teach students the importance of effective data visualization in data science. Students will explore Seaborn, a Python library for intuitive statistical graphics, and Altair, a declarative visualization library for Python. They will also have the opportunity to create impactful visualizations with real datasets through hands-on exercises. "},{"uri":"https://aaubs.github.io/ds23/en/m1/","title":"Applied Data Science and Machine Learning","tags":[],"description":"","content":"M1 - Applied Data Science and Machine Learning Note: M1 - Final Assignment Deadline: Friday, 6 October 2023, 12:00 PM\nThis module provides a condensed introduction to the “Data Science Pipeline,” introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication. This module includes the following four main topics:\nA) Introduction to Data Science B) Rapid Prototyping C) Unsupervised Machine Learning D) Supervised Machine Learning "},{"uri":"https://aaubs.github.io/ds23/en/m1/02_rapid_prototyping/01.5_streamlit_offline/","title":"Streamlit Development &amp; Running Offline","tags":[],"description":"","content":"Streamlit provides an easy and rapid way to turn data scripts into interactive web apps. However, at times, developers may need to ensure that their Streamlit applications run offline. Here\u0026rsquo;s a simple guide to setting up Streamlit in an offline environment using Anaconda and Visual Studio Code (VSCode).\nSetup with Anaconda Environment Follow these steps to set up Streamlit within an Anaconda environment:\nInstall Anaconda \u0026amp; VSCode\nDownload and install Anaconda. Download and install VSCode. Create a Python Conda Environment\nconda create --name bds-streamlit python==3.11.4 Activate the Environment\nconda activate bds-streamlit Install pipreqs\npipreqs is a handy tool that scans through your project files, detects the Python imports, and subsequently generates a requirements.txt file. pip install pipreqs Generate requirements.txt\npipreqs Install the Necessary Packages\npip install -r requirements.txt Alternative Setup without Anaconda Environment If you\u0026rsquo;d rather not use a specific environment:\nInstall Anaconda \u0026amp; VSCode (as mentioned above).\nInstall the Required Packages Directly\npip install pandas streamlit matplotlib altair seaborn Running the Streamlit App Navigate to Your Project Directory\ncd path/to/your/app_directory Run the App\nstreamlit run app.py Alternatively (especially for Windows users), you can execute: python -m streamlit run app.py With these steps, you\u0026rsquo;re all set to develop and run your Streamlit apps offline! Whether you\u0026rsquo;re using an Anaconda environment or not, the process is straightforward and ensures you have all the necessary dependencies installed.\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/01_intro_ds/03_data_visualization_stat/","title":"- Exploratory Data Analysis and Essential Statistics","tags":[],"description":"","content":"This session introduces students how to use EDA and some fundemental concepts of statistical methods to uncover patterns, anomalies, and frame questions in data. Students will learn foundational measures and techniques for data interpretation, and they will have the opportunity to apply EDA and statistical methods on datasets through hands-on exercises.\nDeep Dive into EDA: Uncover patterns, anomalies, and frame questions. Key Statistical Concepts: Foundational measures and techniques for data interpretation. Hands-on Analysis: Apply EDA and statistical methods on datasets. Part 1: Statistics refresher Notebook statistics refresher Part 2: Further concepts Notebook propability distributions Notebook AB testing Further studies Recommended DataCamp courses Intyroduction to statistics (no coding) Statistical Thinking in Python I Statistical Thinking in Python II Statistical Simulation in Python Recommended readings Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python ###€ Further resources\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/02_rapid_prototyping/02_online_dashboard/","title":"- Real World Data to Online Dashboard","tags":[],"description":"","content":"This session will focus on building interactive online dashboards using real-world geospatial data, with an emphasis on GeoPandas for data manipulation and visualization. Students will learn how to use GeoPandas to read, filter, and manipulate geospatial data, create interactive map-based visualizations, and ultimately deploy their dashboards to the web.\nIntroduction to GeoPandas Using GeoPandas to analyze geospatial data will be our focus in these notebooks.\nGeoPandas GeoPandas and Solutions GeoPandas Hands-on Project GeoPandas Hands-on Project and Solutions GeoPandas Exercises GeoPandas exercises GeoPandas exercises and solutions From Geopandas to Streamlit App 🚀 Simplified version of the analysis - with Folium Plotting App code Deployed App What to do now?! Replay code from the course and see if you do understand the core mechanics - you DO NOT need to remember everything. Android app market project on datacamp Course: Python DS toolbox 1 \u0026amp; Course: Python DS toolbox 2 Opendata.dk - build a map of different attractions in Aalborg based on public data. See preprocessing example - how to get data out of nested JSON - below: This is how you can preprocess the GeoCoordinates from the JSON file:\n#Load pandas import pandas as pd # Read the file from remote data = pd.read_json(\u0026#39;https://admin.opendata.dk/dataset/44ecd686-5cb5-40f2-8e3f-b5e3607a55ef/resource/eeabb0f8-1b19-4c80-b059-5ba5c4c872d2/download/guidedenmarkaalborgenjson.json\u0026#39;) # The GeoCoordinates are hiding in the Address column data[\u0026#39;Address\u0026#39;][0][\u0026#39;GeoCoordinate\u0026#39;] # You can use list comprehension to pull out GeoCoordinates (also empty values) - try out # This will allow you to filter for missing data without fancy workarounds [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # Make a new column based on that to be used for filtering out missing data data[\u0026#39;GeoCoordinate\u0026#39;] = [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # drop, where no GeoCoordinate data = data.dropna(subset=[\u0026#39;GeoCoordinate\u0026#39;]) # Pull out the values data[\u0026#39;latitude\u0026#39;] = [x[\u0026#39;Latitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] data[\u0026#39;longitude\u0026#39;] = [x[\u0026#39;Longitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] "},{"uri":"https://aaubs.github.io/ds23/en/m1/02_rapid_prototyping/","title":"B) Rapid Prototyping (W37)","tags":[],"description":"","content":" Note: Dashboard-Hackathon Submission Deadline: Friday, 15 September 2023, 12:00\nThis topic includes 3 sessions as follows:\nRapid Prototyping with Streamlit \u0026amp; Gradio (Monday, September 11th, 10:15-14:15): This session will cover the basics of rapid prototyping in data science, with a focus on the Streamlit and Gradio libraries. Students will learn how to use these libraries to create interactive web applications that can be used to explore data and test hypotheses. Real-World Data to Online Dashboard (Tuesday, September 12th, 12:30-16:15): This session will focus on creating interactive online dashboards using real-world data. Students will learn how to select and clean data, create visualizations, and deploy their dashboards to the web. Dashboard-Hackathon Kick-off (Tuesday, September 12th, 16:30-18:15): This session will kick off the Data Science Dashboard Hackathon. Students will have the opportunity to compete to create the most compelling and user-friendly interactive online dashboard using Streamlit. "},{"uri":"https://aaubs.github.io/ds23/en/info/02_modules/","title":"Modules","tags":[],"description":"","content":"For Business Data Science Students M1: Data Handling, Exploration \u0026amp; Applied Machine Learning 10 ECTS\nThis module will prove a condensed introduction to the “Data Science Pipeline”, introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication.\nM2: Network Analysis and Natural Language Processing 5 ECTS\nFocuses on analyzing a variety of unstructured data sources. Particularly, students will learn how to explore, analyze, and visualize natural language (text) as well as relational (network) data.\nM3: Data-Driven Business Modelling and Strategy 15 ECTS Course with integrated project in which you will learn how companies plan, prepare and execute data-driven projects. In the project you will work wich a company case and build a \u0026ldquo;mini\u0026rdquo; version of the product/process.\nFor Social Data Science Students - Elective Semester M3: (SDS) Deep Learning and Artificial Intelligence for Analytics 5 ECTS\nIntroduces to the most recent developments in machine learning, which are deep learning and artificial intelligence applications. The module will provide a solid foundation for this exciting and rapidly developing field. Students will learn whether and how to apply deep learning techniques for business analytics, and acquire proficiency in new methods autonomously.\nCapstone Project Semester project utilising techniques and approaches from SDS in the context of a problem related to your main study field.\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/03_uml/04_intro_kmeans/","title":"- Introduction to Clustering: K-means and Hierarchical Approaches","tags":[],"description":"","content":"This session will introduce the principles and applications of clustering. Students will learn about the different types of clustering problems, and they will explore some of the most popular clustering algorithms, such as K-means and hierarchical clustering.\nNotebook(s) Hands-on Intro Clustering App NBA Player Injury Replacement Recommender [ NBA Player Injury Replacement Recommender Streamlit] (https://injuryreplacement-9dqxgbmk9zbfqveaxqfqtd.streamlit.app/) Recommended Datacamp exercises: Python Recommended Readings and resources Python Data Science Handbook Chapter 5 What Is Machine Learning? Introducing Scikit-Learn Feature Engineering In Depth: Principal Component Analysis In Depth: k-Means Clustering "},{"uri":"https://aaubs.github.io/ds23/en/m1/03_uml/","title":"C) Intro to Unsupervised Machine Learning (W38)","tags":[],"description":"","content":" Note: Unsupervised Machine Learning Assignment Submission Deadline: Friday, 22 September 2023, 12:00\nThis topic includes 5 sessions as follows:\nIntroduction to Unsupervised Machine Learning (Mon, Sep 18th, 08:15-12:00): This session will dive into the foundational concepts and real-world uses of unsupervised machine learning (UML). As part of this, students will gain insights into various UML challenges. Furthermore, they will explore notable UML algorithms, including PCA, SVD, NMF, and an introduction to clustering via k-means. UML 2: Recommendation \u0026amp; Similarity Search (Tue, Sep 19th, 12:30-16:15): This session will focus on the application of UML to recommendation systems and similarity search. Students will learn about the different types of recommendation systems, and they will explore how UML can be used to improve the accuracy and relevance of recommendations. Building Recommender Systems with Gradio (Tue, Sep 19th, 16:30-18:15): This session will be a hands-on workshop where students will build their own recommender systems using the Gradio interface and UML methods. Students will work in teams to create a recommender system for a real-world problem. Clustering extended: K-means \u0026amp; Hierarchical Approaches (Wed, Sep 20th, 08:15-10:00): This session will introduce the principles and applications of clustering. Students will learn about the different types of clustering problems, and they will explore some of the most popular clustering algorithms, such as K-means and hierarchical clustering. **Demo session: Present your Streamlit App. All groups pitch their APP (5min - pitch). After that prices are given to the winning team and runner up. 🥳 UML Clustering Group Exercise (Wed, Sep 20th, 12:30-14:15): This session will be a hands-on exercise where students will cluster a real-world dataset using UML methods. Students will work in teams to identify the optimal number of clusters for the dataset, and they will analyze the characteristics of each cluster. "},{"uri":"https://aaubs.github.io/ds23/en/info/04_litetrature/","title":"Literature &amp; Resources","tags":[],"description":"","content":"While this course does not come with a list of mandatory readings, we will often refer to some central resources in R and python, which for the most part can always be accessed in a free and updated online version. We generally recommend you to use these amazing resources for problem-solving and further self-study on the topic.\nMain Literature These pieces of work can be seen as main references for data science using Python. We will frequently refer to selected chapters for further study. Documentation of the used packages, tutorials, papers, podcasts etc. will be added throughout.\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O\u0026rsquo;Reilly Media, Inc. Online available here Wilke, C. O. (2019). Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O\u0026rsquo;Reilly Media. Supplementary literature Essential Math for Data Science O\u0026rsquo;Reilly Media. Nield, T. (2022): Math refresher targeting data science relevant concepts. Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python Documentation of packages/libraries used Note: Papers, Business Cases, Videos, Tutorials, Podcasts, and Blogposts will be presented and assigned during the course.\nFurther Ressources Data Science Cloud services Notebook bases: Google Colab: Googles popular service for editing, running \u0026amp; sharing Jupyter notebooks (Only Python Kernel, but R kernel can be accessed via some tricks) Deepnote: New popular online notebook service with good integration to other services (Python, R \u0026amp; more) Kaggle: Also provides their own cloud-based service co create and run computational notebooks. Convenient, unlimited, but a bit slow (Pyhton, r ). Instance based: UCloud: New cloud infrastructure provided by AAU, AU, SDU AAU Strato: AAU CLAUDIA infratructure. Very powerful, but access needs a bit of experience with working via terminal. Community Kaggle: Crowdsourced data science challanges. Nowadays also provides a vivid community where you find datasets, notebooks for all kind of data science exercises. madewithml Tools \u0026amp; Helpers "},{"uri":"https://aaubs.github.io/ds23/en/info/03_schedule/","title":"Semester Schedule","tags":[],"description":"","content":"This will be shortly updated with additional key dates and topics for the semester. For now, please follow CalMoodle.\nGeneral appointments Introduction to Semester Project and group formation: 24.10.2023, 14:30-16:00 M1: Week 35-41 Topics W 35: Introduction \u0026amp; landing W 36: Data Manipulation, Exploratory Data Analysis (EDA) W 37: Exploratory Data Analysis (EDA) / Dashboard development / Hackathon W 38: Unsupervised Machine Learning (UML), Math for ML W 39: Supervised Machine Learning (SML) W 40: Group Assignment W 41: Exam Key Dates Data Storytelling Hackathon\nIn groups: Developing EDA Dashboard 12.09.2023 - 15.09.2023 Group assignment: 29.09.-06.10.2023 (Digital Eksamen)\nFinal exam: 10-11.09.2023\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/01_intro_ds/05_data_visualization_ds/","title":"- Data Visualization in Data Science","tags":[],"description":"","content":"This session will teach you the fundamentals of data visualization in data science. You will learn the importance of effective data visualization, the principles that drive meaningful visuals, and how to use two popular Python libraries for data visualization: Seaborn and Altair.\nFoundations of Visualization: Understand the importance of effective data visualization in Data Science and the principles that drive meaningful visuals. Seaborn Mastery: Explore Seaborn, a Python library for intuitive statistical graphics. Altair Exploration: Delve into Altair, a declarative visualization library for Python. Hands-on Visualization: Harness both Seaborn and Altair to craft impactful visualizations with real datasets. Notebooks Notebook Dataviz Slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides here\n"},{"uri":"https://aaubs.github.io/ds23/en/info/05_requirements_project/","title":"Semester Project Requirements","tags":[],"description":"","content":"Format Functional and self-contained notebook Happy to see GitHub repos (which you can use as your portfolio in the job market) Project report (30-ish pages - max. 45) Some study relation (but that is debatable and not necessarily required) Report is a (semi/non) technical documentation. Think about a corporate censor that you try to inform Content Problem formulation with some practical and theoretical motivation (no huge literature discussion) Methodology (not a critical realist vs positivist discussion but some ideas about what can be concluded potentially) Data sourcing and pre-processing strategy Overall architecture of the model(s) Modelling (incl. finetuning) Results Discussion / Conclusion Scope Uses different methods from the course (at least 2 modules) in a creative way Downloading data from kaggle/github and running an ML model is probably not enough for a good performance Creative combinations of methodologies, please: combine financial data with social media data to look at equity development extract information from text data and create networks. Use network indicators to supplement company data Evaluation will focus on correct application and communication of DS methods The level of \u0026ldquo;technicality\u0026rdquo; is as in the course with emphasis on application and intuition, not on ML engineering / mathematics However, you will need to demonstrate insight into statistics on a level that is required to discuss your assignment e.g. interpret and discuss performance indicators, outline strategies for improvement e.g. under/oversampling "},{"uri":"https://aaubs.github.io/ds23/en/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://aaubs.github.io/ds23/en/","title":"Social / Business Data Science 2023","tags":[],"description":"","content":"Social \u0026amp; Business Data Science 2023 Aalborg University Business School The corresponding Aalborg University Moodle course page can be found here. Note that for updated content this page rather than Moodle will be used. At AAUBS Data Science we believe in the power of open science and open education. Following AAU’s “Knowledge for the world” strategy, we aim at making our material available outside password protected university systems.\n"},{"uri":"https://aaubs.github.io/ds23/en/tags/","title":"Tags","tags":[],"description":"","content":""}]