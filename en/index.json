[{"uri":"https://aaubs.github.io/ds23/en/m4/01_intro-to-deep-learning/1_basicsdl/","title":"Basics Deep Learning","tags":[],"description":"","content":" This session will cover the basics of deep learning, including its history, important concepts, and applications. We will review model architectures and parameters, including the loss function and optimizer. We will also cover training regimes and the basics of gradient descent and backpropagation. Literature: LeCun, Y., Bengio, Y., \u0026amp; Hinton, G. (2015)\nNotebooks Basics Deep Learning Slides Use arrows keys on keyboard to navigate. Alternatively use fullscreen slides.\n"},{"uri":"https://aaubs.github.io/ds23/en/m2/01_networks/1_networks/","title":"Basics Network Analysis","tags":[],"description":"","content":"This session introduces basic concepts of network theory and analysis.\nRecommended Datacamp exercises: Introduction to Network Analysis in Python Notebooks Basics Network Analysis Intermediate Network Analysis Slides Use arrows keys on keyboard to navigate. Alternatively use fullscreen slides\n"},{"uri":"https://aaubs.github.io/ds23/en/m2/02_nlp/1-nlp-intro-sml/","title":"Basics of NLP","tags":[],"description":"","content":" Corgies doing NLP - Medieval Fresco. 2022. Roman x Stable Diffusion\nThis session introduces basic concepts of NLP. We will take a Problem-Based-Learning approach and I will introduce NLP-related concepts as we go. If you need a more theoretical intro (standalone), I\u0026rsquo;ll uploaded pre-recorded videos. We will start with a project based on\nContext This assignment is based on data from this paper\nDavidson, T., Warmsley, D., Macy, M., \u0026amp; Weber, I. (2017, May). Automated hate speech detection and the problem of offensive language. In Proceedings of the international AAAI conference on web and social media (Vol. 11, No. 1, pp. 512-515).\nYou are given a collection of approximately 25k tweets that have been manually (human) annotated. class denotes: 0 - hate speech, 1 - offensive language, 2 - neither\nhttps://github.com/SDS-AAU/SDS-master/raw/master/M2/data/twitter_hate.zip\n1. Preprocessing and vectorizaion. Justify your choices and explain possible alternatives (e.g. removing stopwords, identifying bi/tri-grams, removing verbs or use of stemming, lemmatization etc.)\nCreate a bag-of-words representation, apply TF-IDF and dimensionality reduction (LSA-topic modelling alternatively simply PCA or SVD) to transform your corpus into a feature matrix. 2. Explore and compare the 2 \u0026ldquo;classes of interest\u0026rdquo; - hate speech vs offensive language. Can you see differences by using simple count-based approaches? Can you identify themes (aka clusters / topics) that are specific for one class or another? Explore them using, e.g. simple crosstabs - topic vs. class and to get more detailed insights within-cluster top (TF-IDF) terms. (This step requires preprocessed/tokenized inputs). 3. Build an ML model that can predict hate speech Use the ML pipeline (learned in M1) to build a classification model that can identify offensive language and hate speech. It is not an easy task to get good results. Experiment with different models on the two types of text-representations that you create in 2.\nBonus: Explore missclassified hate speech tweets vs those correctly predicted. Can you find specific patterns? Can you observe some topics that are more prevalent in those that the model identifies correcly?\nThe best-reported results for this dataset are.\nClass Precision 0 0.61 1 0.91 2 0.95 Overall 0.91 Notebook This notebook contains an extended solution.\nHate Speech Detection Context - Exercise: Presidential Debate 2020 Yes, we are going back in time to the Presidential Debate in the US 2020 - the time of lots of unhappy Tweeting. It\u0026rsquo;s just too good a dataset and case to let it go\u0026hellip;\nData Political tweets: https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pol_tweets.gz from https://github.com/alexlitel/congresstweets We\u0026rsquo;ve preprocessed a bit to make things easier. 1: Dems. 0: Rep.\nTweets around the time of the debate in oktober 20 (8000): https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pres_debate_2020.gz\nBoth datasets are in JSON format. Task: Build a classifier that can distinguish Dem/Rep tweets. Bonus: 1. Explore discussed topics; 2. find out what drives predictions.\nNotebook In-class-solution and add-ons (TM)\nPolitical Tweets Prediction "},{"uri":"https://aaubs.github.io/ds23/en/m1/01_basics/01_stat_prog/","title":"Basics Statistical Programming","tags":[],"description":"","content":"\nThis session is a basic introduction to statistical programming as well as a short brush-up on data more generally. For some, this will be \u0026ldquo;old news\u0026rdquo;, but many will certainly benefit from reviewing this material. We start with a general theory lecture on data structures and properties, and then dive into Python specific applications of statistical programming.\nPart 1: Basics Notebook: Python 101 Part 2: Basic Data Manipulation Notebook: Python Data Manipulation Exercises Notebook: Python DS Handbook, C.2-3 exercises\nNotebook: Pandas exercises\nHere, you will find the answers to the exercises:\nNotebook: Python DS Handbook, C.2-3 exercises and solutions\nNotebook: Pandas exercises and solutions\nFurther studies Recommended DataCamp courses Introduction to Python Intermediate Python Recommended readings Python fo Data Science Handbook (VanderPlas, 2016), Chapter 2-3 Further resources "},{"uri":"https://aaubs.github.io/ds23/en/m4/02_traditional-neural-network-architectures/1_cnns/","title":"CNNs","tags":[],"description":"","content":" This session will focus on convolutional neural networks (CNNs), which are commonly used in image and video analysis. We will discuss the basics of CNNs, how they work, and how they can be applied to business problems. Literature LeCun, Y., Bengio, Y., \u0026amp; Hinton, G. (2015)\nNotebooks Neural network architectures 2: CNNs Slides Neural network architectures 2: CNNs "},{"uri":"https://aaubs.github.io/ds23/en/m6/01_databases/","title":"Databases","tags":[],"description":"","content":"\nIn this section, we will introduce databases and related concepts. We will start with classical relational datab ases and the associated Structual Query Language, and then glimpse a bit into NoSQL (Not only SQL) database concepts.\nRecommended datacamp exercises. Relational databases (SQL) Introduction to SQL intermediate SQL NoSQL NoSQL Concepts (no coding) Introduction to MongoDB "},{"uri":"https://aaubs.github.io/ds23/en/m4/04_transformer/1_basictm/","title":"Fine-tuning transformers","tags":[],"description":"","content":"\nResources You will need a HFü§ó account to upload models We will use WandB for monitoring - get an account if you like to follow that part Notebooks Transformer-based Translation (inference)\nFinetuning Sequence Classification with Bert\nHomework - As a preparation for final assignment‚ÄºÔ∏è Finetune a model for token-classification Follow this tutorial Cheating option 1: NERDA, Thanks to the lovely people at Ekstrabladet/PIN Cheating option 2: Simpletransformers Try custom Named Entities Skills, Scientific Terms Make your own NER dataset using Argilla and 0-shot labeling with Flair - you can deploy argilla with 1-click on ü§ó spaces and connect to it via e.g. Colab. Nerd-out options GPT-2 finetuning. Make it produce text in a certain style. Follow this tutorial Build a retreival based Question-Answering system with [ü¶ú‚õìÔ∏èLangChain][https://langchain.readthedocs.io/en/latest/] "},{"uri":"https://aaubs.github.io/ds23/en/m6/03_from-notebook-to-api/1_-introduction-to-from-notebook-to-api/","title":"From notebook to API","tags":[],"description":"","content":"Code refactoring for production: In this section, students will learn how to refactor their code from a notebook environment to a production-ready API.\nIntroduction to FastAPI: FastAPI is a modern, fast, and easy-to-use API framework. In this section, students will learn the basics of FastAPI and how to build and deploy an API using it.\nSlides Code Refractoring Slides API Lecture Slides "},{"uri":"https://aaubs.github.io/ds23/en/m6/05_deployment/1_introduction-deployment-docker-cloudrun/","title":"From notebook to containarized app in the cloud","tags":[],"description":"","content":"Creating the Functionality This tutorial guides you through building a simple cloud-based containerized app that provides entity anonymization functionality. The app is built using the FastAPI framework and Spacy library. The main functionality of the app is to take user text input, identify entities in the text using Spacy\u0026rsquo;s pre-trained model, and then return the same text with the identified entities replaced by \u0026lsquo;X\u0026rsquo;. The app includes a simple \u0026lsquo;get\u0026rsquo; route that returns a message to confirm that the server is running. By following this tutorial, you will learn how to create a basic app using FastAPI, how to use Spacy for named entity recognition, and how to containerize the app using Docker.\nCheck out the repository here https://github.com/RJuro/anonymize_bds\nthe core function takes incoming text, identifies entities using spacy, and XXs them out using their start and end positions (given by spacy).\ndef anonymize_text(text: str, model): # Use the Spacy model to parse the input text and extract named entities doc = model(text) # Create a list of dictionaries representing the identified entities entities = [ { \u0026#34;start\u0026#34;: ent.start_char, # start position of the entity in the text \u0026#34;end\u0026#34;: ent.end_char, # end position of the entity in the text \u0026#34;type\u0026#34;: ent.label_, # type of the entity (e.g., PERSON, ORGANIZATION, etc.) \u0026#34;text\u0026#34;: ent.text, # actual text of the entity } for ent in doc.ents # iterate over each named entity in the document ] # Create a copy of the input text as a list of characters anonymized_text = list(deepcopy(text)) # Iterate over each identified entity and replace its characters with \u0026#39;X\u0026#39; for entity in entities: start = entity[\u0026#34;start\u0026#34;] # start position of the entity in the text end = entity[\u0026#34;end\u0026#34;] # end position of the entity in the text anonymized_text[start:end] = \u0026#34;X\u0026#34; * (end - start) # replace the entity\u0026#39;s characters with \u0026#39;X\u0026#39; # Convert the list of characters back into a string anonymized_text = \u0026#34;\u0026#34;.join(anonymized_text) # Return a dictionary containing the identified entities and the anonymized text return {\u0026#34;entities\u0026#34;: entities, \u0026#34;anonymized_text\u0026#34;: anonymized_text} when going from use of SpaCy in notebooks to an app, it is a good idea to have the model in the project directory. This can be done with nlp.to_disk('models/en').\nCreating the API To turn the function into a FastAPI route, we need to add a few things (not too many)\n@app.post(\u0026#34;/anonymize\u0026#34;, response_model=EntitiesOut) def extract_entities(user_request: UserRequestIn): We define the route but also the response_model and user_request using pydantic BaseModel.\nRunning the API locally The API can run locally using uvicorn:\nuvicorn app:app\nWith that you can deploy the app on a virtual machine, considering requirements etc. By the way: Create a requirements.txt using pipreqs. Don\u0026rsquo;t forget to add uvicorn to the requirements.\nDockerizing the app You can dockerize the app using a simple dockerfile in the same repository.\n# Use the official Python 3.9 image as the base image for the new Docker image FROM python:3.9 # Copy the contents of the current directory (the `.`) to the `/api` directory inside the Docker image COPY . /api # Install the Python packages listed in the `requirements.txt` file located in the `/api` directory RUN pip install -r /api/requirements.txt # Set the `PYTHONPATH` environment variable to the root directory inside the Docker container ENV PYTHONPATH=/ # Set the working directory inside the Docker container to the root directory WORKDIR / # Expose port 8000 on the Docker container EXPOSE 8000 # Specify that the entry point for the Docker container is the `uvicorn` command ENTRYPOINT [\u0026#34;uvicorn\u0026#34;] # Specify the default command to run when the container starts. This command starts the Uvicorn server, serving the `app` object in the `api.app` module, listening on all available network interfaces (`0.0.0.0`) CMD [\u0026#34;api.app:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;] It is important to be careful with the relative paths. When loading the SpaCy model in the script for instance model = spacy.load('api/models/en') make sure to reference the model path relative to the pythonpath specified when creating the image.\nYou can now run the image using docker docker build -t app . the -t app is just giving it the name app. Once the container is running, you can deploy the API.\nPush to Github and deploy You can push the project folder to Github. Then go to goolge cloud platform (in this tutorial), select Cloud Run and the repository used. Make sure to provide the correct path to the dockerfile, which Cloud Run will use to create an Image. Important: don\u0026rsquo;t forget to change the standard port 8080 to 8000. Cloud Run should than take care of the rest, creating the image and running your API in the serverless and scalable environment.\n"},{"uri":"https://aaubs.github.io/ds23/en/m5/02_presentations/","title":"In-calss Presentations","tags":[],"description":"","content":"Presentation format Students will deliver 20-25 minute presentations on AI, safety, and ethics, inspired by assigned documents. They will also develop 2-3 questions or interactive formats for class engagement. Topics include politics and safety, planning for AGI, and issues like bias, toxicity, and misinformation in large language models. Presentations may be published on a GitHub page with accompanying blog posts.\n"},{"uri":"https://aaubs.github.io/ds23/en/info/","title":"Info, Schedule &amp; Co","tags":[],"description":"","content":"General info about the semester will be updated here. Check out the calendar on moodle.\nIntro to the semester and module: "},{"uri":"https://aaubs.github.io/ds23/en/info/01_infrastructure/","title":"Infrastructure","tags":[],"description":"","content":"Main Infrastructure MS teams : Join our MS our teams channel to get updates from us, Q\u0026amp;A, and talk to your peers. Moodle : We will not use it too much, but for the sake of completeness. However, you will still find your calendar there and we may use it from time to time to send out mass-mails to you all. Datacamp : Get access to all the Datacamp premium content for free to facilitate your data science journey. Use your AAU mail here when signing up. Adittional Infrastructure used Github Provides internet hosting for software development and version control using Git. It offers the distributed version control and source code management (SCM) functionality of Git, plus its own features. It is also commonly used to host open-source projects, including data science projects. If you do not have it already, you are advice to create an account to manage and showcase your work during this semester. Getting started Jupyter/Colab Intro Python Colab Github Hello World The Markdown Guide: Introduction to markdown, the formating language used in ipython notebooks. "},{"uri":"https://aaubs.github.io/ds23/en/m4/01_intro-to-deep-learning/","title":"Intro to Deep Learning","tags":[],"description":"","content":"This chapter introduces you to the basics of deep learning, including its history, important concepts, and applications.\nLiterature Literature: LeCun, Y., Bengio, Y., \u0026amp; Hinton, G. (2015) Recommended Datacamp exercises Deep Learning with PyTorch "},{"uri":"https://aaubs.github.io/ds23/en/m6/04_mlops-with-mlflow/1_introduction-mlops-with-mlflow/","title":"Introduction MLOps with mlflow","tags":[],"description":"","content":"MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for tracking experiments, packaging and sharing code, and deploying models.\nMLflow provides a unified platform for managing the entire machine learning lifecycle, from experimentation to deployment. It helps to increase productivity, collaboration, and reproducibility in data science projects.\nHere are the main components of MLflow:\nExperiment Tracking: This component helps you to track your machine learning experiments by recording and visualizing metrics, parameters, and artifacts. It allows you to easily compare different runs and reproduce results.\nModel Packaging: This component provides a simple format for packaging data science code in a reusable and reproducible way. It also allows you to specify dependencies, such as libraries and data files, and to run code in different environments. This component provides a standardized way to package and deploy machine learning models. It supports a variety of popular frameworks, such as TensorFlow, PyTorch, and Scikit-learn, and provides tools for deploying models to a variety of platforms, such as Docker containers and cloud services.\nModel Registry: This component provides a centralized repository for managing and sharing machine learning models. It allows you to track model versions, assign permissions, and share models with other users.\nNotebooks MLflow Basic MLOps workflow using MLflow - Into Basic MLOps workflow using MLflow - Functions Basic MLOps workflow using MLflow - Google Drive and Ngrok Basic MLOps workflow using MLflow - SQLite and Ngrok CI/CD Continuous Integration and Continuous Deployment - Into Continuous Integration and Continuous Deployment - GitHub Action and Heroku "},{"uri":"https://aaubs.github.io/ds23/en/m6/02_big-data-workflows/1_-introduction-to-big-data-workflows/","title":"Introduction to Big Data workflows","tags":[],"description":"","content":" In ML projects, it is often necessary to process large amounts of data, known as Big Data. This can pose a significant challenge for traditional data processing tools and techniques. Fortunately, there are a variety of Big Data processing engines available today that are designed to handle these challenges, such as Apache Spark and Polars. Spark is a powerful Big Data processing engine that allows you to work with large datasets in ML projects. It provides a flexible and scalable platform for data processing, allowing you to process data in real-time or in batch mode, and enabling you to run complex algorithms and models on massive datasets.\nPolars is another Big Data processing engine that is gaining popularity due to its speed, scalability, and ease of use. It is an open-source data processing library designed for processing large, complex datasets with ease. It provides a Pandas-like API for data manipulation, enabling users to easily perform a wide range of data processing tasks, such as filtering, aggregating, and transforming data.\nBoth Spark and Polars offer powerful solutions for handling Big Data in ML projects, and choosing the right engine largely depends on the specific needs and requirements of your project. Regardless of which engine you choose, however, both Spark and Polars have revolutionized the way that we process and analyze large datasets, and have enabled us to unlock valuable insights and knowledge from data that was previously impossible to handle.\nNotebooks Polars Performing a Big Data workflow with Pandas and Polars Performing a Big Data workflow with Polars - Into Polars Exercises Polars Exercises - Solutions Spark Performing a Big Data workflow with Spark - Into Performing a Big Data workflow with Spark - Part 2 Performing a Big Data workflow with Spark - Part 3 Spark Exercises Spark Exercises - Solutions "},{"uri":"https://aaubs.github.io/ds23/en/m6/01_databases/01_databases/","title":"Introduction to databases","tags":[],"description":"","content":"\nIn this lecture, we will provide brief examples how to populate and query SQL and NoSQL databases. We will also demonstrate how to use them in a ML workflow.\nRessources Colab Notebook SQL Intro Colab Notebook NoSQL Intro "},{"uri":"https://aaubs.github.io/ds23/en/m6/05_deployment/2_introduction-to-docker-and-deploying-scalable-ml/","title":"Introduction to Docker and deploying scalable ML","tags":[],"description":"","content":"Docker is an open-source platform that simplifies the development, deployment, and management of applications. It enables developers to create, package, and distribute applications as lightweight containers that can run consistently across various environments.\nMachine Learning (ML) models often require specific dependencies, libraries, and configurations to function properly. Docker simplifies the deployment of ML models by creating an isolated environment that contains all the necessary dependencies.\nIn this session, we\u0026rsquo;ll introduce Docker, its main concepts, and how to deploy scalable ML models using Docker containers.\nDocker Basics a. Dockerfile: A text file containing instructions and commands for building a Docker image. It defines the environment, dependencies, and configurations required for the application. b. Images: A Docker image is a lightweight, stand-alone, executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Images are built from a series of layers, each representing a set of instructions defined in a Dockerfile. c. Containers: A container is a lightweight, stand-alone, and executable software package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. Deploying Scalable ML Models Docker can be combined with orchestration tools like Kubernetes or Docker Swarm to deploy scalable ML models. These tools help manage, scale, and distribute Docker containers across multiple nodes or clusters. This allows you to handle high loads and serve multiple requests concurrently, ensuring a reliable and efficient service.\nNotebooks Docker Essential Docker Commands for MLOps Step-by-Step Guide to Installing Docker for MLOps Dockerizing MLOps on AWS: Simplifying Deployment and Scalability with Docker Persist data in a container app "},{"uri":"https://aaubs.github.io/ds23/en/m4/03_intro-to-transformer-models/1_basictm/","title":"Introduction to transformers","tags":[],"description":"","content":"\nLiterature Sutskever, I., Vinyals, O., \u0026amp;amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., \u0026amp;hellip; \u0026amp;amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\nThe illustrated transformer\nSimple transformer LM\nNotebooks Seq2Seq - Neural Machine Translation\nSimple transformer LM\nSlides "},{"uri":"https://aaubs.github.io/ds23/en/m6/1_basicsmlops/","title":"MLOps: A Framework and Maturity Model","tags":[],"description":"","content":"Paper Discussion John, M. M., Olsson, H. H., \u0026amp; Bosch, J. (2021, September). Towards mlops: A framework and maturity model. In 2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA) (pp. 1-8). IEEE\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/03_sml/01-sml-intro/","title":"Supervised Machine Learning (SML)","tags":[],"description":"","content":"This session introduces supervised machine learning (SML)\nNotebook(s) Introduction to Supervised Machine Learning Recommended Datacamp exercises: Basics:\nPython - Intro to supervised learning Python - Decision Tree modeling Modell workflows\nPython - SML Preprocessing Python - SML Feature Engineering Python - SML Model validation Recommended Readings and resources Python Data Science Handbook Chapter 5 "},{"uri":"https://aaubs.github.io/ds23/en/m1/02_uml/01-uml-intro/","title":"Unsupervised Machine Learning (UML) intro","tags":[],"description":"","content":"This tutorial will provide a hands-on intro to core techniques of unsupervised learning. We will cover, data preprocessing, PCA, NMF, UMAP, K-means and methods of interpreting results of their application.\nNotebook(s) Hands-on Intro to Dimensionality reduction and Clustering Recommended Datacamp exercises: Python Recommended Readings and resources Python Data Science Handbook Chapter 5\nWhat Is Machine Learning? Introducing Scikit-Learn Feature Engineering In Depth: Principal Component Analysis In Depth: k-Means Clustering Implementation tutorials on YT PCA and K-means from this list\n"},{"uri":"https://aaubs.github.io/ds23/en/m2/01_networks/2_networks_2mode/","title":"2 Mode Networks","tags":[],"description":"","content":"This session introduces to multimodel (2+) network analysis concepts.\nRecommended Datacamp exercises: Intermediate Network Analysis in Python Notebooks 2-Mode Networks "},{"uri":"https://aaubs.github.io/ds23/en/m1/","title":"Applied Data Science and Machine Learning","tags":[],"description":"","content":"M1 - Applied Data Science and Machine Learning This module provides a condensed introduction to the ‚ÄúData Science Pipeline‚Äù, introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication.\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/01_basics/","title":"Basics of Statistical Programming and Data Manipulation (W35-36)","tags":[],"description":"","content":"This chapter is a basic introduction to statistical programming as well as a short brush-up on data more generally. In addition, we will have some statistics refreshers, reviewing important concepts. For some, this will be \u0026ldquo;old news\u0026rdquo;, but many will certainly benefit from reviewing this material. Afterwards, we introduces some fundamental concepts of data manipulation and exploratory data analysis (EDA).\nIntro slides Slides fullscreen "},{"uri":"https://aaubs.github.io/ds23/en/m6/02_big-data-workflows/","title":"Big Data workflows","tags":[],"description":"","content":"Introduction to Big Data workflows: In ML projects, it is often necessary to process large amounts of data, known as Big Data.\nThis will be demonstrated at the case of Spark, a powerful Big Data processing engine allowing to work with large datasets in ML projects. The introduction will include tasks such as setting up a Spark environment, reading and writing data, and performing transformations and aggregations on data.\nRecommended Datacamp exercises PySpark Documentation Polars Documentation "},{"uri":"https://aaubs.github.io/ds23/en/m6/02_big-data-workflows/2_exercise_bigdata_workflow/","title":"Exercise 2: Performing a Big Data workflow with Spark and Polars","tags":[],"description":"","content":" In this session, we will demonstrate how to use Apache Spark, a powerful Big Data processing engine, for processing large datasets in ML projects. The session will cover tasks such as setting up a Spark environment, reading and writing data, and performing transformations and aggregations on data. Additionally, we will also introduce Polars, a similar data manipulation library for Rust, and compare its features to those of Spark.\nThis session will provide hands-on exercises to reinforce your understanding and skills in working with Spark and Polars for processing big data in ML projects.\nNotebooks Polars Exercises for the Bike Sharing Demand Dataset using Polars Exercises and solutions for the Bike Sharing Demand Dataset using Polars Spark Exercises for the Bike Sharing Demand Dataset using Spark Exercises and solutions for the Bike Sharing Demand Dataset using Spark "},{"uri":"https://aaubs.github.io/ds23/en/m6/04_mlops-with-mlflow/2_mlops_mlflow_aws/","title":"MLOps on AWS: a Hands-On Tutorial","tags":[],"description":"","content":" MLOps (Machine Learning Operations) is the practice of automating the process of deploying, monitoring, and maintaining machine learning models. In this hands-on tutorial, we will walk you through the essential steps of implementing MLOps on AWS using MLflow.\nThis session will provide a hands-on project to reinforce your understanding and skills in working with MLflow on AWS in ML projects.\nNotebooks MLflow An Overview of MLflow: Managing End-to-End Machine Learning Lifecycles MLOps on AWS Ubuntu Terminal Basics: Navigating, Managing Files, and Performing Basic Tasks MLOps on AWS: a Hands-On Project MLOps on AWS: a Hands-On Project - MLflow "},{"uri":"https://aaubs.github.io/ds23/en/m4/01_intro-to-deep-learning/2_ann/","title":"Neural network architectures","tags":[],"description":"","content":"In this session, we will introduce you to artificial neural networks (ANNs) and show how they can be used to solve various business problems. We will also discuss the different types of ANNs and their strengths and weaknesses.\nNotebooks Basics Deep Learning Slides Use arrows keys on keyboard to navigate. Alternatively use fullscreen slides.\n"},{"uri":"https://aaubs.github.io/ds23/en/m4/02_traditional-neural-network-architectures/","title":"Neural network architectures","tags":[],"description":"","content":"The purpose of this chapter is to introduce you to the various types of traditional deep learning, including CNNs, RNNs, and LSTMs, along with their histories, key concepts, and applications.\nIn this course, we will focus on different classical neural network architectures, including artificial neural networks (ANNs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks.\nwe will use real-world business examples to illustrate the concepts and techniques covered in the lectures. By the end of the course, students will have a solid understanding of deep learning and its current applications in business.\nThe students will have frequent opportunities to work in groups on four mini-projects. These projects will involve applying the concepts and techniques learned in the lectures to real-world business examples. The goal of these projects is to give students hands-on experience with deep learning and help them build a portfolio of work to showcase their skills. By the end of the course, each student will have a collection of four mini-projects that demonstrate their ability to apply deep learning to solve business problems.\nLiterature Olah, Christopher. \u0026ldquo;Understanding lstm networks.\u0026rdquo; (2015). Recommended Datacamp exercises Deep Learning with PyTorch "},{"uri":"https://aaubs.github.io/ds23/en/m2/02_nlp/2-nlp-topic-model/","title":"NLP - Other areas","tags":[],"description":"","content":" Corgies doing NLP - Medieval Fresco. 2022. Roman x Stable Diffusion\nThis session will continue with statistical NLP techniques in a \u0026ldquo;less guided fashion\u0026rdquo;.\nPlan for the day Finish up political tweets (dem/rep classifier) Vectorization, distance and similarity for text (maybe even Word2Vec) What research is published at AAUBS? (Scopus + Topic Models) - Hamid takes over. Scopus Topic Models Homework Finish up reseach topic modeling Turning it into an app\u0026hellip; (build \u0026amp; deploy a text classifier app - template code below) Can you beat the score? üéÉüéÉüéÉ Spooky NLP Spooky Author Identification Spooky NLP Notebook import streamlit as st import pandas as pd import numpy as np import pickle import preprocessor as prepro import spacy nlp = spacy.load(\u0026#39;en_core_web_sm\u0026#39;) from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression import eli5 st.set_page_config( page_title=\u0026#34;Hate-Finder üîç\u0026#34;, page_icon=\u0026#34;üîç\u0026#34;) # write everything into one function that can be re-used later def text_prepro(texts): \u0026#34;\u0026#34;\u0026#34; takes in a pandas series (1 column of a DF) removes twitter stuff lowercases, normalizes text \u0026#34;\u0026#34;\u0026#34; texts_clean = texts.map(lambda t: prepro.clean(t)) texts_clean = texts_clean.str.replace(\u0026#39;#\u0026#39;,\u0026#39;\u0026#39;) clean_container = [] for text in nlp.pipe(texts_clean, disable=[\u0026#34;tagger\u0026#34;, \u0026#34;parser\u0026#34;, \u0026#34;ner\u0026#34;]): txt = [token.lemma_.lower() for token in text if token.is_alpha and not token.is_stop and not token.is_punct] clean_container.append(\u0026#34; \u0026#34;.join(txt)) return clean_container @st.experimental_singleton def load_model(): pipe = pickle.load(open(\u0026#39;hate_pipe.pkl\u0026#39;,\u0026#39;rb\u0026#39;)) return pipe pipe = load_model() txt = st.text_area(\u0026#39;Text to analyze\u0026#39;, \u0026#39;\u0026#39;\u0026#39; Write here some (potentially hate-speech) text \u0026#39;\u0026#39;\u0026#39;) if st.button(\u0026#39;Predict if HATE üòµ\u0026#39;): to_analyse = text_prepro(pd.Series([txt])) result = pipe.predict(to_analyse)[0] st.write([\u0026#39;HATE!\u0026#39;,\u0026#39;Not nice!\u0026#39;,\u0026#39;All good!\u0026#39;][result]) "},{"uri":"https://aaubs.github.io/ds23/en/m2/01_networks/3_exercises/","title":"NW Exercises","tags":[],"description":"","content":"Exercise 1: Manager Networks Introduction In this exercise, you will replicate a well known network analysis, with different data and some twists. Data: The data is to be found at: https://github.com/SDS-AAU/SDS-master/tree/master/00_data/network_krackhard (Hint: You neet to download the raw data) Data: What do I get? Background Let the fun begin. You will analyze network datacollected from the managers of a high-tec company. This dataset, originating from the paper below, is widely used in research on organizational networks. Time to give it a shot as well. Krackhardt D. (1987). Cognitive social structures. Social Networks, 9, 104-134. The company manufactured high-tech equipment on the west coast of the United States and had just over 100 employees with 21 managers. Each manager was asked to whom do you go to for advice and who is your friend, to whom do you report was taken from company documents. Description\nThe dataset includes 4 files - 3xKrack-High-Tec and 1x High-Tec-Attributes. Krack-High-Tec includes the following three 21x3 text matrices:\nADVICE, directed, binary FRIENDSHIP, directed, binary REPORTS_TO, directed, binary Column 1 contains the ID of the ego (from where the edge starts), and column 2 the alter (to which the edge goes). Column 3 indicates the presence (=1) or absence (=0) of an edge.\nHigh-Tec-Attributes includes one 21x4 valued matrix.\nID: Numeric ID of the manager AGE: The managers age (in years) TENURE: The length of service or tenure (in years) LEVEL: The level in the corporate hierarchy (coded 1,2 and 3; 1 = CEO, 2 = Vice President, 3 = manager) DEPT: The department (coded 1,2,3,4 with the CEO in department 0, ie not in a department) Tasks 1. Create a network Generate network objects for the companies organizational structure (reports to), friendship, advice This networks are generated from the corresponding edgelists Also attach node characteristics from the corresponding nodelist 2. Analysis Make a little analysis on:\nA: Network level characteristics. Find the overal network level of:\nDensity Transistivity (Clustering Coefficient) Reciprocity \u0026hellip; for the different networks. Describe and interpret the results. Answer the following questions:\nAre relationships like friendship and advice giving usually reciprocal? Are friends of your friends also your friends? Are the employees generally more likely to be in a friendship or advice-seeking relationship? B: Node level characteristics: Likewise, find out:\nWho is most popular in the networks. Who is the most wanted friend, and advice giver? Are managers in higher hirarchy more popular as friend, and advice giver? C: Relational Characteristics: Answer the following questions:\nAre managers from the same 1. department, or on the same 2. hirarchy, 3. age, or 4. tenuere more likely to become friends or give advice? (hint: assortiativity related) Are friends more likely to give each others advice? 3. Visualization Everything goes. Show us some pretty and informative plots. Choose what to plot, and how, on your own. Interpret the results and share some insights.\nSolution :::: HERE :::: Exercise 2: Crime Network Data In this exercise, you will analyze a crime network, which you will find in the following notebook: Notebook: crime network Tasks First of all, construct the 2-mode network (code provided). Then, project it on th two modes, so that you end up with:\nA network with persons as nodes, connected by jointly commited crimes A network with crimes as nodes, connected by persons jointly commiting them. Then, try to solve th following tasks:\nfind the crime(s) that have the most shared connections with other crimes find the individual(s) that have the most shared connections with other individuals Which persons are implicated in the most number of crimes? Then, there is also a bonus exercise which is a bit more complicated:\nFind the people that can help with investigating a crime\u0026rsquo;s person: Let\u0026rsquo;s pretend that we are a detective trying to solve a crime, and that we right now need to find other individuals who were not implicated in the same exact crime as an individual was, but who might be able to give us information about that individual because they were implicated in other crimes with that individual. "},{"uri":"https://aaubs.github.io/ds23/en/m4/02_traditional-neural-network-architectures/2_rnns-lstms/","title":"RNNs and LSTMs","tags":[],"description":"","content":" In this session, we will introduce students to recurrent neural networks (RNNs) and long short-term memory (LSTM) networks. We will discuss the basics of RNNs and LSTMs, how they differ from other neural network architectures, and how they can be applied to business problems.\nSlides Use arrows keys on keyboard to navigate. Alternatively use fullscreen slides.\n"},{"uri":"https://aaubs.github.io/ds23/en/m4/03_intro-to-transformer-models/2_sbert/","title":"Sentence-Transformers SBERT","tags":[],"description":"","content":"\nSentence Transformers are a recent breakthrough in natural language processing that can generate dense, high-quality embeddings of sentences, enabling accurate semantic similarity comparisons between sentences. What makes them particularly exciting for businesses and social science applications is their ability to enable more intuitive, meaningful language-based search, content deduplication, and clustering. With Sentence Transformers, businesses can enhance the accuracy of their search engines, provide more accurate recommendations, and reduce redundancy in content databases. Social science researchers can use Sentence Transformers to identify commonalities between texts and to cluster documents to identify trends and topics in large corpora.\nResources OG SBERT-Paper Reimers, N., \u0026amp;amp; Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.\nSBERT Docu\nNLP with SBERT - an ebook/course on the use of dense vectors (with SBERT for business applications)\nSBERT-Training Tutorial\nBERTopic - a framework for topic modelling with SBERT embeddings\nMilvus - Vector database\nSlides Notebooks Semantic Search\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/01_basics/02_intro_stats/","title":"Statistics Refresher","tags":[],"description":"","content":" Theory part is general, Python application part hands-on and language specific.\nThis session is a basic introduction and refresher of statistical concepts important to data science.\nPart 1: Statistics refresher Slides Notebookm statistics refresher Part 2: Further concepts Notebook propability distributions Notebook AB testing Further studies Recommended DataCamp courses Intyroduction to statistics (no coding) Statistical Thinking in Python I Statistical Thinking in Python II Statistical Simulation in Python Recommended readings Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python ###‚Ç¨ Further resources\nx "},{"uri":"https://aaubs.github.io/ds23/en/m1/03_sml/02-sml-ts/","title":"Time Series Forecasts","tags":[],"description":"","content":"This session introduces to time series analysis and forecasting\nNotebook(s) Introduction to Timeseries Forecasting Recommended Datacamp exercises: Python - Time Series Analysis Python - SML for Time Series Data Recommended Readings and resources \u0026hellip; "},{"uri":"https://aaubs.github.io/ds23/en/m4/04_transformer/2_tst_intro/","title":"Timeseries Transformers intro","tags":[],"description":"","content":"In this segment, we will review deep learning based approaches to timeseries forecasting. We will focus on timeseries transformers, particularly Google\u0026rsquo;s Temporal Fusion transformers.\nRessources TFT colab TFT Paper "},{"uri":"https://aaubs.github.io/ds23/en/m1/02_uml/02-uml-recommender/","title":"UML Appliations - Recommender Systems","tags":[],"description":"","content":" Corgis in an Airport. 2022. Roman x Stable Diffusion\nIn this workshop we are going to learn about recommender systems as a type of UML. Such systems are probably the most widely used and commercialy valuable form of AI today. Specifically we will be looking into collaborative filtering and matrix factorization.\nPlan for today Collaborative filtering / SVD recommender using Nomadlist Trips-data in a Notebooks Streamlit recommender-app Notebook(s) Uncommented code-snippet notebook App source code for the app deployed recommender app Recommended Readings and resources This excellent PyData Talk by the developer of LightFM Recommended Datacamp exercises: Beginner Tutorial RecSys-Course "},{"uri":"https://aaubs.github.io/ds23/en/m1/01_basics/03_data_visualization/","title":"Basics Data Visualization","tags":[],"description":"","content":"This session introduces some fundamental concepts of data visualization. After a theoretical lecture on types and dimensions of data visualization, we will explore the visualization of different data types, structure, and properties in R and Python specific applications.\nNotebooks Notebook Dataviz Slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides here\n"},{"uri":"https://aaubs.github.io/ds23/en/m6/01_databases/2_exercise_poc_db_backend/","title":"Building PoC with DB backend","tags":[],"description":"","content":"Welcome to this Exercise session on using TinyDB and SQLite, two lightweight databases that are ideal for small-scale applications and development tasks. While TinyDB is a document-oriented database that emphasizes simplicity and ease of use with JSON-like structures, SQLite is a popular and lightweight SQL-based relational database management system.\nThroughout this tutorial, we will explore the following topics:\nLoading a CSV file into both TinyDB and SQLite Performing basic operations, including SELECT, INSERT, UPDATE, and DELETE, in both databases Utilizing these databases for machine learning projects We will be using Python along with the TinyDB and SQLite libraries to accomplish these tasks. By the end of this tutorial, you will have a solid understanding of the fundamental concepts and operations in both TinyDB and SQLite. Additionally, we will provide a hands-on exercise using Google Colab to help you practice and reinforce the skills you have learned.\nNotebooks Building PoC with DB_backend SQLite Building PoC with DB_backend SQLite - Solution Building PoC with DB_backend TinyDB Building PoC with DB_backend TinyDB - Solution "},{"uri":"https://aaubs.github.io/ds23/en/m4/01_intro-to-deep-learning/3_exercise_ann_pytorch/","title":"Exercise Session 1","tags":[],"description":"","content":"In this session, you will review PyTorch and learn how to train, infer, and evaluate a simple ANN.\nAuto Insurance in Sweden Swedish Committee on Analysis of Risk Premium in Motor Insurance. read more\nOverview In the dataset:\nX = number of claims Y = total payment for all the claims in thousands of Swedish Kronor for geographical zones in Sweden Reference: Swedish Committee on Analysis of Risk Premium in Motor Insurance\nTasks First step: We will create a simple Artificial Neural Network with 1 node and training with 1 sample of data Second step: The simple Artificial Neural Network will be trained through the dataset Notebooks Implementing a Simple ANN using PyTorch Implementing Simple ANN using PyTorch - Solution Resources PyTorch PyTorch NN Linear "},{"uri":"https://aaubs.github.io/ds23/en/m4/02_traditional-neural-network-architectures/3_exercise-session-2_pytorch/","title":"Exercise Session 2","tags":[],"description":"","content":"In this session, you will review PyTorch and learn how to train, infer, and evaluate RNNs and LSTMs.\nStarbucks Stock Data Starbucks Corporation is an American multinational chain of coffeehouses and roastery reserves headquartered in Seattle, Washington. As the world\u0026rsquo;s largest coffeehouse chain, Starbucks is seen to be the major representation of the United States\u0026rsquo; second wave of coffee culture. As of September 2020, the company had 32,660 stores in 83 countries, including 16,637 company operated stores and 16,023 licensed stores. Of these 32,660 stores, 18,354 were in the United States, Canada, and Latin America. Starbucks locations serve hot and cold drinks, whole-bean coffee, micro-ground instant coffee, espresso, caffe latte, full and loose-leaf teas, juices, Frappuccino beverages, pastries, and snacks. Some offerings are seasonal or specific to the locality of the store. Depending on the country, most locations offer free Wi-Fi. Headquartered in the Starbucks Center, the company was founded in 1971 by Jerry Baldwin, Zev Siegl, and Gordon Bowker at Seattle\u0026rsquo;s Pike Place Market. During the early 1980s, they sold the company to Howard Schultz who ‚Äì after a business trip to Milan, Italy ‚Äì decided to make the coffee bean store a coffeeshop serving espresso-based drinks. While chief executive officer from 1986 to 2000, Schultz\u0026rsquo;s first tenure led to an aggressive expansion of the franchise, first in Seattle, then across the West Coast of the United States (Content from Wikipedia).\nTasks First step: We will create RNNs and LSTMs Second step: RNNs and LSTMs will be trained through the dataset Notebooks Here you will find the notebooks for this session:\nImplementing RNNs using PyTorch Implementing RNNs using PyTorch - Exercise Implementing LSTMs using PyTorch Implementing LSTMs using PyTorch - Exercise Implementing LSTMs using PyTorch - Solution Implementing LSTMs using PyTorch - Text Generation Implementing LSTMs using PyTorch - Text Generation with embeddings layer Resources PyTorch PyTorch NN Linear "},{"uri":"https://aaubs.github.io/ds23/en/m4/03_intro-to-transformer-models/3_exercise-session-3_pytorch/","title":"Exercise Session 3","tags":[],"description":"","content":"In this session, you will learn how to finetune SBERT embeddings and use them in downstream tasks. They will also practice using SBERT in a variety of business scenarios.\nHate Speech Dataset Online hate speech on social media networks can influence hate violence and even crimes against a certain group of people in this digital age. According to FBI statistics, hate-related attacks on specific groups of people are at a 16-year high [1]. Due to this, there is a growing need to eradicate hate speech through automatic detection to reduce the burden on moderators Datasets were obtained from Reddit and a white supremacist forum, Gab, where human-labeled comments are classified as hate speech [2].\nOverview The dataset used for this project consists of Tweets labeled as hate_speech, offensive_language, or neither. In the dataset:\ncount = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF). hate_speech = number of CF users who judged the tweet to be hate speech. offensive_language = number of CF users who judged the tweet to be offensive. neither = number of CF users who judged the tweet to be neither offensive nor non-offensive. class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither Tasks First step: Performance comparison of Transformer models and traditional embedding models Second step: SBERT for semantic similarity (Patent Search using PatentSBERTa) Notebooks Here you will find the notebooks for this session:\nCore contents SetFit Hatespeech vs bert and distilroberta\nSBERT for Patent Search using PatentSBERTa in PyTorch\nClassification with various vectorization approaches TF-IDF and W2V Multi-Class Text Classification BERT Multi-Class Text Classification Implementing Multi-Class Text Classification LSTMs using PyTorch Add-on Implementing LSTMs using PyTorch - Text Generation Basic LSTMs using PyTorch "},{"uri":"https://aaubs.github.io/ds23/en/m4/04_transformer/3_exercise-session-4_pytorch/","title":"Exercise Session 4","tags":[],"description":"","content":"In this session, you will learn how to upload your fine-tuned HuggingFace model and deploy it using two different platforms: Streamlit and Gradio. First, you will learn how to properly upload your model to the HuggingFace Hub, a platform that allows you to easily share and access models. Then, you will learn how to use Streamlit and Gradio, two user-friendly platforms, to deploy your model and make it accessible to others. By the end of this session, you will be able to confidently upload and deploy your models for use by others.\nHow to upload your fine-tuned model on HuggingFace First, it logs in to the Hugging Face Hub using notebook_login(), which allows for easy uploading and access to Hugging Face models.\n#Importing the necessary libraries from huggingface_hub import notebook_login from transformers import pipeline #Login to Hugging Face account from the Jupyter notebook notebook_login() Next, it pushes the trainer object, model object, and dataset object to the Hub using their push_to_hub() methods. This uploads these objects to the Hub for others to use and access.\n#Pushing the trained model, trainer and dataset to the Hugging Face Hub trainer.push_to_hub(\u0026#39;Your Fine-Tuned HuggingFace Model\u0026#39;) model.push_to_hub(\u0026#39;Your Fine-Tuned HuggingFace Model\u0026#39;) dataset.push_to_hub(\u0026#39;Your Fine-Tuned HuggingFace Model\u0026#39;) Finally, the code loads a pre-trained Danish emotion classification model using model_ckpt = \u0026lsquo;Your Fine-Tuned HuggingFace Model\u0026rsquo;, and creates a pipeline using the pipeline() method from the transformers library. This pipeline can then be used to classify text inputs into one of several emotional categories.\n#Defining the path to the trained model checkpoint on the Hugging Face Hub model_ckpt = \u0026#39;Your Fine-Tuned HuggingFace Model\u0026#39; #Creating a pipeline for example text classification using the fine-tuned model from the Hugging Face Hub pipe = pipeline(\u0026#39;text-classification\u0026#39;, model=model_ckpt) #Passing the Danish text examples to the pipeline and getting the predicted labels pipe([\u0026#39;Du er en idiot!\u0026#39;, \u0026#39;Jeg er sulten!\u0026#39;]) How to deploy your fine-tuned model on HuggingFace You can also deploy your fine-tuned Hugging Face model on Streamlit through the Hugging Face web interface. Here are the steps to do so:\nAfter uploading your fine-tuned Hugging Face model on the Hugging Face Model Hub, create a new space in your Hugging Face profile and enter the details for your app, including the name of your application. Select \u0026ldquo;Streamlit\u0026rdquo; (or Gradio) as the deployment type. click \u0026ldquo;New space\u0026rdquo;. Upload your app.py and requirements.txt files to the Hugging Face web interface. Your app is automatically deployed. That\u0026rsquo;s it! Your fine-tuned Hugging Face model is now deployed on Streamlit through the Hugging Face web interface and can be accessed using your app.\nTwo Simple Examples for Deploying an App using Gradio and Streamlit 1. Danish text sentiment analysis using Gradio:\n# app.py import gradio as gr from transformers import pipeline from transformers import AutoTokenizer, AutoModelForSequenceClassification #Defining the classify function which takes text as input and returns the label of the sentiment def classify(text): # Initializing the pipeline for sentiment analysis cls = pipeline(\u0026#39;text-classification\u0026#39;, model=\u0026#39;RJuro/dk_emotion_bert_in_class\u0026#39;) # Predicting the sentiment label for the input text return cls(text)[0][\u0026#39;label\u0026#39;] #Creating the Gradio interface with input textbox and output text gr.Interface(fn=classify, inputs=[\u0026#34;textbox\u0026#34;], outputs=\u0026#34;text\u0026#34;).launch() # requirements.txt transformers gradio torch 2. Hugging Face Sentiment Analysis Spaces Example using Streamlit:\n# app.py import streamlit as st from transformers import pipeline from textblob import TextBlob pipe = pipeline(\u0026#39;sentiment-analysis\u0026#39;) st.title(\u0026#34;Hugging Face Sentiment Analysis Spaces Example\u0026#34;) st.subheader(\u0026#34;What framework would you like to use for Sentiment Analysis\u0026#34;) #Picking what NLP task you want to do option = st.selectbox(\u0026#39;Framework\u0026#39;,(\u0026#39;Transformers\u0026#39;, \u0026#39;TextBlob\u0026#39;)) #option is stored in this variable #Textbox for text user is entering st.subheader(\u0026#34;Enter the text you\u0026#39;d like to analyze.\u0026#34;) text = st.text_input(\u0026#39;Enter text\u0026#39;) #text is stored in this variable if option == \u0026#39;Transformers\u0026#39;: out = pipe(text) else: out = TextBlob(text) out = out.sentiment st.write(\u0026#34;Sentiment of Text: \u0026#34;) st.write(out) # requirements.txt torch transformers textblob How to use Inference APIs for connecting to HuggingFace models Using Openai API and Requests to connect HuggingFace models Assignment for part 4 Build an exctiting and perhapps also fun application using techniques learned in this module Submission: Github repo as usual.\nMinimal requirements: relevant task solved self-trained or fine-tuned transformer, however not sentence transformer for semantic search only (you are welcome to explore techniques beyond the scope of the course e.g. on HF) published on HF gradio (in-notebook) app or HF spaces Nice-to: Streamlit app on Hub optional use of API (HF Inference API, Cohere, be careful with OpenAI üí∏) optional more complex LLM setup with e.g. langchain, promptify, pinecone and other integrations etc. ‚ÄºÔ∏è Submission 13/3 noon Submission also via DigitalExam, where you compile all your previous assignments and submit in one file for the overall portfolio for the module exam. You are welcome to tweak/improve previous module submissions for that. "},{"uri":"https://aaubs.github.io/ds23/en/m6/03_from-notebook-to-api/","title":"From notebook to API","tags":[],"description":"","content":"Code refactoring for production: In this section, students will learn how to refactor their code from a notebook environment to a production-ready API.\nIntroduction to FastAPI: FastAPI is a modern, fast, and easy-to-use API framework. In this section, students will learn the basics of FastAPI and how to build and deploy an API using it.\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/01_basics/03-intro-eda/","title":"Hands-on data manipulation and EDA ","tags":[],"description":"","content":" Corgi working on a Data Science project. 2022. Roman x Stable Diffusion\nIn this session we will be trying out some of the techniques already learned on real world data from AirBnb as well as experiment with Kaggle\nPart 1: AirBnb In this notebook we will be using data from AirBnb for some basic EDA and geoplotting\nEDA and Geoviz starter EDA and Geoviz class Part 2: Kaggle In this notebook we will be learning how to work with data from Kaggle as well as exercise more simple data-viz.\nKaggle starter Kaggle class What to do now?! Replay code from the course and see if you do understand the core mechanics - you DO NOT need to remember everything. Android app market project on datacamp Course: Python DS toolbox 1 \u0026amp; Course: Python DS toolbox 2 Opendata.dk - build a map of different attractions in Aalborg based on public data. See preprocessing example - how to get data out of nested JSON - below: This is how you can preprocess the GeoCoordinates from the JSON file:\n#Load pandas import pandas as pd # Read the file from remote data = pd.read_json(\u0026#39;https://admin.opendata.dk/dataset/44ecd686-5cb5-40f2-8e3f-b5e3607a55ef/resource/eeabb0f8-1b19-4c80-b059-5ba5c4c872d2/download/guidedenmarkaalborgenjson.json\u0026#39;) # The GeoCoordinates are hiding in the Address column data[\u0026#39;Address\u0026#39;][0][\u0026#39;GeoCoordinate\u0026#39;] # You can use list comprehension to pull out GeoCoordinates (also empty values) - try out # This will allow you to filter for missing data without fancy workarounds [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # Make a new column based on that to be used for filtering out missing data data[\u0026#39;GeoCoordinate\u0026#39;] = [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # drop, where no GeoCoordinate data = data.dropna(subset=[\u0026#39;GeoCoordinate\u0026#39;]) # Pull out the values data[\u0026#39;latitude\u0026#39;] = [x[\u0026#39;Latitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] data[\u0026#39;longitude\u0026#39;] = [x[\u0026#39;Longitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] Introduction to GeoPandas Using GeoPandas to analyze geospatial data will be our focus in this notebook.\nGeoPandas GeoPandas exercises "},{"uri":"https://aaubs.github.io/ds23/en/m4/03_intro-to-transformer-models/","title":"Intro to transformer models","tags":[],"description":"","content":"The purpose of this chapter is to introduce you to transformer models, the core idea of attention, and milestone transformer architectures. We will also cover the tooling used in the transformer ecosystem, including libraries like sbert, transformers, and simpletransformers.\nThroughout the course, we will apply transformers to real-world business problems. This will include using sbert for sentence and image embedding, finetuning and inference of common transformer-based language models like BERT, and training timeseries transformers for time-sensitive business applications.\nwe will use real-world business examples to illustrate the concepts and techniques covered in the lectures. By the end of the course, students will have a solid understanding of transformer models and its current applications in business.\nThe students will have frequent opportunities to work in groups on four mini-projects. These projects will involve applying the concepts and techniques learned in the lectures to real-world business examples. The goal of these projects is to give students hands-on experience with deep learning and help them build a portfolio of work to showcase their skills. By the end of the course, each student will have a collection of four mini-projects that demonstrate their ability to apply deep learning to solve business problems.\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/02_uml/","title":"Intro to Unsupervised Machine Learning (W38)","tags":[],"description":"","content":"This chapter will be a general intro to unsupervised learning, covering theoretical concepts and practical use of different algorithms for dimensionality reduction and clustering.\nIntro slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides\n"},{"uri":"https://aaubs.github.io/ds23/en/info/02_modules/","title":"Modules","tags":[],"description":"","content":"For Business Data Science Students M1: Data Handling, Exploration \u0026amp; Applied Machine Learning 10 ECTS\nThis module will prove a condensed introduction to the ‚ÄúData Science Pipeline‚Äù, introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication.\nM2: Network Analysis and Natural Language Processing 5 ECTS\nFocuses on analyzing a variety of unstructured data sources. Particularly, students will learn how to explore, analyze, and visualize natural language (text) as well as relational (network) data.\nM3: Data-Driven Business Modelling and Strategy 15 ECTS Course with integrated project in which you will learn how companies plan, prepare and execute data-driven projects. In the project you will work wich a company case and build a \u0026ldquo;mini\u0026rdquo; version of the product/process.\nFor Social Data Science Students - Elective Semester M3: (SDS) Deep Learning and Artificial Intelligence for Analytics 5 ECTS\nIntroduces to the most recent developments in machine learning, which are deep learning and artificial intelligence applications. The module will provide a solid foundation for this exciting and rapidly developing field. Students will learn whether and how to apply deep learning techniques for business analytics, and acquire proficiency in new methods autonomously.\nCapstone Project Semester project utilising techniques and approaches from SDS in the context of a problem related to your main study field.\n"},{"uri":"https://aaubs.github.io/ds23/en/m2/01_networks/","title":"Network Analysis","tags":[],"description":"","content":"This chapter introduces you to network analysis and working with relational data.\n"},{"uri":"https://aaubs.github.io/ds23/en/m2/","title":"Network Analysis &amp; NLP","tags":[],"description":"","content":"M2 - Network Analysis \u0026amp; NLP This module provides a condensed introduction analyzing two popular forms of unstructured data, namely relational and text data.\nContent by week for this module Click on the to do for the week to see what you should do to keep up with the module\nW 41: Introduction to Network Analysis W 43: Introduction to Natural-Language-Processing (NLP) W 44: Advanced Applications in Network and Text Analysis W 45: Project work + exam "},{"uri":"https://aaubs.github.io/ds23/en/m2/02_nlp/3-nlp-advanced/","title":"NLP Applications Chatbot","tags":[],"description":"","content":" Roman x Stable Diffusion\nChatbots are one of the most wide spread NLP applications. In this tutorial we will build a simple retrieval chatbot that can be used for example as an alternative for FAQ applications in companies.\nThe approach is following:\nTrain a model on variants of a question. Take input and predict the type of question asked - this is called \u0026ldquo;intent\u0026rdquo; Reply with a pre-defined response corresponding to the question asked. Modern bots are more complex. They evaluate the whole (or large parts of the) dialogue. In addition some have the capacity to generate text.\nThe iput data looks like this:\nüëâ Expand to see JSON... { \u0026#34;intents\u0026#34;: [ { \u0026#34;tag\u0026#34;: \u0026#34;greeting\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;Hi\u0026#34;, \u0026#34;Hey\u0026#34;, \u0026#34;How are you\u0026#34;, \u0026#34;Is anyone there?\u0026#34;, \u0026#34;Hello\u0026#34;, \u0026#34;Good day\u0026#34; ], \u0026#34;responses\u0026#34;: [ \u0026#34;Hey :-)\u0026#34;, \u0026#34;Hello, thanks for visiting\u0026#34;, \u0026#34;Hi there, what can I do for you?\u0026#34;, \u0026#34;Hi there, how can I help?\u0026#34; ] }, { \u0026#34;tag\u0026#34;: \u0026#34;goodbye\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;Bye\u0026#34;, \u0026#34;See you later\u0026#34;, \u0026#34;Goodbye\u0026#34;], \u0026#34;responses\u0026#34;: [ \u0026#34;See you later, thanks for visiting\u0026#34;, \u0026#34;Have a nice day\u0026#34;, \u0026#34;Bye! Come back again soon.\u0026#34; ] }, { \u0026#34;tag\u0026#34;: \u0026#34;thanks\u0026#34;, \u0026#34;patterns\u0026#34;: [\u0026#34;Thanks\u0026#34;, \u0026#34;Thank you\u0026#34;, \u0026#34;That\u0026#39;s helpful\u0026#34;, \u0026#34;Thank\u0026#39;s a lot!\u0026#34;], \u0026#34;responses\u0026#34;: [\u0026#34;Happy to help!\u0026#34;, \u0026#34;Any time!\u0026#34;, \u0026#34;My pleasure\u0026#34;] }, { \u0026#34;tag\u0026#34;: \u0026#34;items\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;Which items do you have?\u0026#34;, \u0026#34;What kinds of items are there?\u0026#34;, \u0026#34;What do you sell?\u0026#34; ], \u0026#34;responses\u0026#34;: [ \u0026#34;We sell coffee and tea\u0026#34;, \u0026#34;We have coffee and tea\u0026#34; ] }, { \u0026#34;tag\u0026#34;: \u0026#34;payments\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;Do you take credit cards?\u0026#34;, \u0026#34;Do you accept Mastercard?\u0026#34;, \u0026#34;Can I pay with Paypal?\u0026#34;, \u0026#34;Are you cash only?\u0026#34; ], \u0026#34;responses\u0026#34;: [ \u0026#34;We accept VISA, Mastercard and Paypal\u0026#34;, \u0026#34;We accept most major credit cards, and Paypal\u0026#34; ] }, { \u0026#34;tag\u0026#34;: \u0026#34;delivery\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;How long does delivery take?\u0026#34;, \u0026#34;How long does shipping take?\u0026#34;, \u0026#34;When do I get my delivery?\u0026#34; ], \u0026#34;responses\u0026#34;: [ \u0026#34;Delivery takes 2-4 days\u0026#34;, \u0026#34;Shipping takes 2-4 days\u0026#34; ] }, { \u0026#34;tag\u0026#34;: \u0026#34;funny\u0026#34;, \u0026#34;patterns\u0026#34;: [ \u0026#34;Tell me a joke!\u0026#34;, \u0026#34;Tell me something funny!\u0026#34;, \u0026#34;Do you know a joke?\u0026#34; ], \u0026#34;responses\u0026#34;: [ \u0026#34;Why did the hipster burn his mouth? He drank the coffee before it was cool.\u0026#34;, \u0026#34;What did the buffalo say when his son left for college? Bison.\u0026#34; ] } ] } Notebook Training Chatbot\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/03_sml/03-sml-addon/","title":"SML - Further topics","tags":[],"description":"","content":"This session introduces adittional topics of intertest in SML\nNotebook(s) Introduction to Explainable ML Recommended Datacamp exercises: \u0026hellip; Recommended Readings and resources \u0026hellip; "},{"uri":"https://aaubs.github.io/ds23/en/m4/","title":"Applied Deep Learning and Artificial Intelligence","tags":[],"description":"","content":"M4 - Applied Deep Learning and Artificial Intelligence This course is an applied introduction to deep learning for business students. We will cover the basics of deep learning, including its foundations, tensors, typical architectures, and applications. We will also discuss the different training paradigms and tooling used in deep learning.\nIn the first half of the course, we will focus on different classical neural network architectures, including artificial neural networks (ANNs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks. In the second half of the course, we will introduce students to transformer models, the core idea of attention, and milestone transformer architectures. We will also cover the tooling used in the transformer ecosystem, including libraries like sbert, transformers, and simpletransformers.\nThroughout the course, we will apply transformers to real-world business problems. This will include using sbert for sentence and image embedding, finetuning and inference of common transformer-based language models like BERT, and training timeseries transformers for time-sensitive business applications.\nLikewise, we will use real-world business examples to illustrate the concepts and techniques covered in the lectures. By the end of the course, students will have a solid understanding of deep learning and its current applications in business. We will have frequent opportunities to work in groups on four mini-projects. These projects will involve applying the concepts and techniques learned in the lectures to real-world business examples. The goal of these projects is to give you hands-on experience with deep learning and help them build a portfolio of work to showcase your skills..\nContent for this module Part 1: Intro to Deep Learning Part 2: Traditional neural network architectures Part 3: Intro to transformer models Part 4: Training and publishing transformer models "},{"uri":"https://aaubs.github.io/ds23/en/m5/","title":"Data Governance and Ethics","tags":[],"description":"","content":"\nThis module explores ethical, legal, and policy aspects of data use in business, covering law, privacy, bias, and fairness. Students will learn to navigate challenges and opportunities in data-driven business models while developing practical skills in algorithmic bias and aligning objectives. The module includes two legal workshops, five AI ethics seminars, student presentations, and requires an individual essay portfolio submission.\nLiterature Your Privacy Is Important to Us! ‚Äì Restoring Human Dignity in Data-Driven Marketing, Jan Trzaskowski (2021), https://www.extuto.com/#978-87-420-0042-7 (ordered for you to the univ. bookstore. You will need a printed copy!) "},{"uri":"https://aaubs.github.io/ds23/en/m1/01_basics/04-eda-open-policing/","title":"EDA workshop &amp; Working with JSON/APIs/MongoDB","tags":[],"description":"","content":"In this workshop we are going to explore 2 things. More advanced EDA as well as working with internet data.\nEDA - Open Policing Project In this workshop we are going to work with the Open Policing Dataset. Below you will find the starter notebook where this dataset (Rhode Island) is explored. You can check out some of the research based on this data ont the project‚Äôs page.\nColab Notebook\nInternet Data, APIs, JSON and MongoDB Digital nomads and corgis working at a beach cafe in Bali. 2022. Roman x Stable Diffusion\nIn this tutorial we will look into JSON, the probably most common datatype on the internet today and how we can work with it. For the tutorial you will either need to install a local copy of MongoDB or make a free-tier account with Atlas (MongoDB Cloud version).\nColab Notebook\nWhat to do now? Datacamp course: Intro to MongoDB "},{"uri":"https://aaubs.github.io/ds23/en/m4/01_intro-to-deep-learning/4_group_assignment/","title":"Group assignment 1","tags":[],"description":"","content":"Portfolio Exercise 1: Introduction Task Build, train, and evaluate a neural network with Pytorch. It should have minimum 2 hidden layers Experiment with at least 5 different variations of hyperparameters (n layers / neurons, activation function, epochs, optimizers, learning rate etc.). Use gradio to build a simple interactive demo (in the notebook). This should include:\nFeature selection Feature engineering (if necessary) Standard ML preprocessing (if necessary) Train-test split. Defining a neural network architecture in putorch Define a training loop. training the model. Try out different hyperparameters. Evaluate the final model on the test data. Visualize results with Grad.io Data Self choosen dataset from the M1 module last semester. Delivery Create a github repository Save colab notebook in the github. Provide a readme.md with brief description. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds23/en/m4/02_traditional-neural-network-architectures/4_group_assignment/","title":"Group assignment 2","tags":[],"description":"","content":"Portfolio Exercise 2: Introduction Task Build, train, and evaluate 2 special types of networks neural network with Pytorch. A CNN for a spatial prediction problem A RNN or LSTM for a sequential problem Experiment with at least 2 different variations of hyperparameters for each network. Optional: Use gradio to build a simple interactive demo (in the notebook). This should include:\nFeature selection and/or engineering (if necessary) SPreprocessing (if necessary) Train-test split. Defining a neural network architecture in putorch Define a training loop. training the model. Try out different hyperparameters. Evaluate the final model on the test data. Visualize results with Grad.io Data 2 self choosen datasets not used in M4 so far. One dataset on image or other spatial data, eg. the classical MNIST dataset One dataset on timeseries from last semesters M1 timeseries forecast part, or M2 text classification. Delivery Create a github repository (or use the existing one and adapt it) Save colab notebook in the github. Provide a readme.md with brief description. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds23/en/m6/02_big-data-workflows/3_group_assignment/","title":"Group assignment 2","tags":[],"description":"","content":"Portfolio Exercise 2: Introduction Welcome to the Apache Spark or Polars Group assignment! For this assignment, you will be using your previous project as the dataset (or other large datasets of your choice) to process EDA part using Apache Spark or Polars. The goal of this assignment is to help you gain hands-on experience in processing and analyzing big data using Apache Spark or Polars. You will learn how to load and manipulate data with Spark or Polars and perform various transformations and actions on the data. This assignment will give you a better understanding of how Spark/Polars works and how it can be used to handle large datasets efficiently. Good luck and have fun!\nTask Select one of your previous projects or a large dataset of your choice, such as a dataset with millions of rows, and use either Apache Spark or Polars to perform the complete EDA report. The following tasks should be performed:\nLoad the dataset into the platform and perform basic exploratory data analysis (EDA) to understand the structure of the data. This includes checking the dimensions of the dataset, examining the data types, and identifying missing values.\nFilter the data to include only the relevant observations. This can be done by removing missing values or filtering based on certain criteria or conditions that are specific to your dataset.\nAggregate the data to obtain summary statistics or metrics. You should run different aggregations using functions such as filter(), select(), groupby(), etc. For example, you can calculate the mean, median, or mode of certain variables to gain a better understanding of the data.\nJoin the data with another dataset (if available) to perform more complex analysis. This can be done by merging two tables using a common key.\nVisualize the data using either Apache Spark\u0026rsquo;s built-in plotting library or any other visualization tools of your choice. You can plot a histogram, scatter plot, line plot, or any other types of charts that are relevant to your dataset. This will help you to identify trends and patterns in the data and gain insights into the underlying relationships between different variables.\nData Select one of your previous projects or a large dataset of your choice, such as a dataset with millions of rows.\nDelivery Create a github repository (or use the existing one and adapt it) Save colab notebook in the github. Provide a readme.md with brief description. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds23/en/m4/03_intro-to-transformer-models/4_group_assignment-3/","title":"Group assignment 3","tags":[],"description":"","content":"Portfolio Exercise 3: Introduction Task Create something cool üöÄ using SBERT and semantic search or perhaps more?! That\u0026rsquo;s a bit of a vague description but there are many options.\nYou are welcome to use images and CLIP You can also use SetFit for supervised tasks with SBERT models. Here are some projects for inspiration:\nGIF search engine Youtube search some more ideas:\nget some podcast transcripts for a specific topic (or create transcripts with Whisper - search for OpenAI Whisper Colab) Finetune an SBERT model using domain adaptation Embed and build a search engine Build a Gradio app Feel free to choose any of these ideas or come up with your own. The goal is to use SBERT and semantic search (or other techniques) to create something interesting and useful.\nData ü§ó datasets Kaggle make your own Delivery Create a github repository (or use the existing one and adapt it) Create a Gradio demo of the model in inference mode Save colab notebook in the github. Provide a readme.md with brief description. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds23/en/m6/04_mlops-with-mlflow/3_group_assignment/","title":"Group assignment 3","tags":[],"description":"","content":"Welcome to the MLflow group assignment! Use MLflow within one of your previous projects and save structured and unstructured information related to your trained model in SQLite within MLflow. Optionally, prepare a ML app based on three layers as Data, Business, Presentation Layers.\nIntroduction For this assignment, you will be using one of your previous projects to train a machine learning model and track and manage your machine learning experiments using MLflow. The goal of this assignment is to help you gain hands-on experience in using MLflow to manage and track your machine learning experiments, and optionally in preparing an ML app based on three layers to provide a user-friendly interface for interacting with your machine learning model. Good luck and have fun!\nTask Select one of your previous projects that includes a machine learning component and use MLflow to track and manage your machine learning experiments. The following tasks should be performed:\nTrain a machine learning model using the data from your previous project. You can use any machine learning model that is appropriate for your data and problem.\nUse MLflow to track and manage your machine learning experiments. Log the hyperparameters, metrics, and artifacts of your machine learning experiments in MLflow. Save structured and unstructured information related to your trained model in SQLite within MLflow.\nOptionally, prepare an ML app based on three layers (data, business, presentation) to provide a user-friendly interface for interacting with your machine learning model. This will involve creating a data layer that handles the data processing pipeline and provides functions for loading and preprocessing the data, a business layer that implements the machine learning model and its related functions, and a presentation layer that implements the user interface and connects it to the business layer.\nData Select one of your previous projects that includes a machine learning component.\nDelivery Create a github repository (or use the existing one and adapt it) Save the Colab notebook or the project folder to GitHub Provide a readme.md with brief description. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds23/en/m6/05_deployment/3_group_assignment/","title":"Group assignment 4","tags":[],"description":"","content":"Title Dockerizing an MLflow-based ML App with SQLite and Streamlit Interface (Optionally, a Three-layer App), and Uploading to Docker Hub\nIntroduction In this assignment, you will be utilizing one of your previous machine learning projects and enhancing it by incorporating MLflow for experiment tracking and management. Additionally, you will integrate SQLite for storing structured and unstructured information related to your trained model. You will also develop a user-friendly interface for your ML app using Streamlit, with the option to create a custom three-layer app. Lastly, you will dockerize your application and upload your dockerized app to Docker Hub.\nTask Choose a previous project that involves a machine learning component and perform the following tasks:\nTrain a machine learning model using the data from your previous project. Select an appropriate machine learning model based on your data and problem.\nIntegrate MLflow for tracking and managing your machine learning experiments. Log hyperparameters, metrics, and artifacts of your experiments in MLflow. Save structured and unstructured information related to your trained model in SQLite within MLflow.\nDevelop a user-friendly interface for your ML app using Streamlit. Optionally, you can create a three-layer ML app (data, business, presentation) for a user-friendly interface to interact with the machine learning model.\nDockerize your ML app, ensuring that the SQLite database, MLflow, and the Streamlit or custom interface are all functioning correctly within the Docker image.\nUpload your dockerized app to Docker Hub and provide instructions for running the app from the Docker Hub repository.\nData Choose one of your previous projects that includes a machine learning component.\nDelivery Create a github repository (or use the existing one and adapt it) Save the Colab notebook or the project folder to GitHub Provide a README.md file with a brief description of your project and instructions for running the app from Docker Hub. Submission can be in groups up to 3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds23/en/info/04_litetrature/","title":"Literature &amp; Resources","tags":[],"description":"","content":"While this course does not come with a list of mandatory readings, we will often refer to some central resources in R and python, which for the most part can always be accessed in a free and updated online version. We generally recommend you to use these amazing resources for problem-solving and further self-study on the topic.\nMain Literature These pieces of work can be seen as main references for data science using Python. We will frequently refer to selected chapters for further study. Documentation of the used packages, tutorials, papers, podcasts etc. will be added throughout.\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O\u0026rsquo;Reilly Media, Inc. Online available here Supplementary literature Essential Math for Data Science O\u0026rsquo;Reilly Media. Nield, T. (2022): Math refresher targeting data science relevant concepts. Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python Further Ressources Data Science Cloud services Notebook bases: Google Colab: Googles popular service for editing, running \u0026amp; sharing Jupyter notebooks (Only Python Kernel, but R kernel can be accessed via some tricks) Deepnote: New popular online notebook service with good integration to other services (Python, R \u0026amp; more) Kaggle: Also provides their own cloud-based service co create and run computational notebooks. Convenient, unlimited, but a bit slow (Pyhton, r ). Instance based: UCloud: New cloud infrastructure provided by AAU, AU, SDU AAU Strato: AAU CLAUDIA infratructure. Very powerful, but access needs a bit of experience with working via terminal. Community Kaggle: Crowdsourced data science challanges. Nowadays also provides a vivid community where you find datasets, notebooks for all kind of data science exercises. madewithml Tools \u0026amp; Helpers "},{"uri":"https://aaubs.github.io/ds23/en/m6/04_mlops-with-mlflow/","title":"MLOps","tags":[],"description":"","content":"MLOps (Machine Learning Operations) is a set of practices and tools used to manage the lifecycle of machine learning models in production environments. It is an extension of DevOps (Development Operations) and aims to bring the same level of automation, collaboration, and agility to machine learning development and deployment.\nML is revolutionizing the world by modernizing industries like healthcare, education, transportation, food, entertainment, and various assembly lines, among others. However, some of the bitter truths of the ML world, when taken to production are:\nDeploying machine learning models to production takes much more time than creating them.\nIn real-world machine learning systems, the actual code that does the machine learning work is only a small part of the overall system. The infrastructure surrounding the code in the production environment is complex and extensive.\nHistorically, ~85 % of ML models that are built never reach production. Surveys \u0026amp; reports also suggest that only ~60% of projects make it from prototype to production ‚Äî that too at organizations that have a decent experience with AI. Wikipedia defines MLOps as:\nMLOps is the process of taking an experimental Machine Learning model into a production system.\nReferences No, You Don‚Äôt Need MLOps Hidden Technical Debt in Machine Learning Systems The Only Truly Hard Problem in MLOps "},{"uri":"https://aaubs.github.io/ds23/en/m2/02_nlp/","title":"Natural Language Processing","tags":[],"description":"","content":"This chapter introduces you to statistical natural language processing. We will be focusing on NLP in combination with supervised ML as well as topic modelling for unsupervised approaches.\nRecommended Datacamp Courses:\nPython Intro to NLP Python - also good Feature Engineering for NLP in Python Recommended Readings:\nBird, S., Klein, E., \u0026amp; Loper, E. (2009). Natural language processing with Python: analyzing text with the natural language toolkit. \u0026quot; O\u0026rsquo;Reilly Media, Inc.\u0026quot;. Docs and linked papers: Gensim Docs Spacy Pre-recorded theory intro recording from last year, in case you\u0026rsquo;d like to review\nNLP intro - level of analysis Text representation From BoW to Topic Modeling and Embeddings (optional) History of NLP in Industry - Yoav Goldberg "},{"uri":"https://aaubs.github.io/ds23/en/m2/02_nlp/4-nlp-embeddings/","title":"NLP Word Vectors","tags":[],"description":"","content":" Roman x Stable Diffusion\nWord embeddings such as Word2Vec or fastText are probably the last techology in NLP before the deep learning revolution. Others would argue that they paved the way for DL becoming the NLP standard. May that as it be, it\u0026rsquo;s a powerfull approach and in this tutorial you will learn about the theory behind these word representations, how they are trained (also from disk). We will also explore approaches to representing text using pretrained vectors.\nBy the way: Embeddings can also be used non-sequential inputs. Here a cool project where recipes are used to create food-vectors: Food2Vec\nWe are actually also going to work with food related data\u0026hellip;\nNotebook Training WordVectors\n"},{"uri":"https://aaubs.github.io/ds23/en/m2/01_networks/4-tutorials-elites-eu-ai-companies/","title":"NW Cases","tags":[],"description":"","content":"Danish Power Elites Many people dream of being one of them, but only few make it all the way to the top. According to two CBS researchers, it takes more than just hard work to get to the top of the Danish hierarchy of power. read more\nIn this project we are going to construct and explore a network of Danish power elites derived from boards of various organisations in th country. We will construct an association network: Who is being in the same board? And first explore \u0026ldquo;basic\u0026rdquo; centrality indicators. Then identify communities and central persons within those. Finally we look at some \u0026ldquo;fancier\u0026rdquo; interactive network visualisation.\nContext: The Danish Power Elites Antons PhD Thesis Brief Summary of findings (CBS) Journal Paper in Sociology More to be found with googleling\u0026hellip; Data Github (R Repository) Magteliten website Or, easier\u0026hellip; on our github Tasks Who are the most central persons? Communities? What characterizes them? Link up with additional data? European AI Companies The European AI Startup Landscape is a project that collects and catalogues AI-related startups in 5 European countries. We created a dataset based on webpages in the catalogue. In this data, we collected all URLs these companies link to on their websites. This is a typical network structure, which can be explored to answer following questions:\nWhich companies are most central players? Are there prominent suppliers / customers? Are there clusters or other agglomerations? Before venturing into analysis, you will have to prepare the data (filter, clean up etc.)\nWe\u0026rsquo;ve prepared a starter notebook for you to get started: üëâ Starter EU AI companies\nNotebooks Starter EU AI companies Starter EU AI companies Python Magteliten Analysis Resources https://gephi.org/ - Gephi: \u0026ldquo;Photoshop for networks\u0026rdquo; Mapping Controversies "},{"uri":"https://aaubs.github.io/ds23/en/info/03_schedule/","title":"Semester Schedule","tags":[],"description":"","content":"This will be shortly updated with additional key dates and topics for the semester. For now, please follow CalMoodle.\nGeneral appointments Introduction to Semester Project and group formation: 12.10.2022, 10:30-12:00 M1: Week 35-39 Topics W 35: Introduction \u0026amp; landing W 36: Data Manipulation, Exploratory Data Analysis (EDA) W 37: Exploratory Data Analysis (EDA) / Dashboard development / Hackathon W 38: Unsupervised Machine Learning (UML), Math for ML W 39: Supervised Machine Learning (SML) W 40: Group Assignment W 41: Exam Key Dates EDA Hackathon\nIn groups: Developing EDA Dashboard 9.09.2022 - 15.09.2022 Group assignment: 30.09.-05.10.2022 (Digital Eksamen)\nFinal exam: 10-11.09.2022\nM2: Week 40-44 Topics W 41: Introduction to Network Analysis (NW) W 42: Autumn break üéâüçÅ W 43: NW applications, Introduction to Natural-Language-Processing (NLP) W 44: Advanced applications in Network and Text Analysis / Module Assignment group work W 45: Exam Key Dates Group assignment: 31.10.-04.11.2022 (Digital Eksamen) Final exam: 9-10.11.2022 M3 / Semester-Project: Week 45-48 Topics W 41: Kick-off M3 course and semester project (12.10.2022, 10:30-12:00) W 44: Strategy and Business Modelling Workshop W 45: Project Management Workshop Key Dates Semester Project Submission: ~ 21/12. Exam: ~ 3-4 Week in January "},{"uri":"https://aaubs.github.io/ds23/en/m1/03_sml/04-sml-workshop/","title":"SML Appliations - Airbnb Pricing","tags":[],"description":"","content":" Unsplash.com @febiyanr\nIn this workshop we will try to use SML, explainability methods and build an app on top of that üòµ\nNotebook(s) Model Training NB Model Inference NB App and repo Streamlit App üöÄ App Repo The App Embedded\u0026hellip; uCloud vs Streamlit vs ML tutorial "},{"uri":"https://aaubs.github.io/ds23/en/m1/03_sml/","title":"Supervised Machine Learning (W39)","tags":[],"description":"","content":"This chapter introduces you to the intuition behind supervised machine (SML) learning, and demonstrates common techniques and workflows. It also contrasts SML (trained on labeled data), from traditional inferential statistics (e.g. econometrics).\nIntro slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides\n"},{"uri":"https://aaubs.github.io/ds23/en/m4/04_transformer/","title":"Transformer models in 2023","tags":[],"description":"","content":"In Part 4 of this course, students will learn about training and publishing transformer models in 2023. Session 7 will cover the finetuning and inference of common transformer-based language models like BERT, and will include classification and token classification tasks such as named entity recognition. Session 8 will focus on using transformer-based models for time series data, including training timeseries transformers and making predictions on data like stock prices or sales data. In the exercise session, students will train and publish their own transformer model, while the group assignment will involve finetuning a transformer model for a business application of their choice, publishing the model on HF, and building a gradio demo.\nLiterature Recommended Datacamp exercises "},{"uri":"https://aaubs.github.io/ds23/en/m6/","title":"Data Engineering and MLOps","tags":[],"description":"","content":"\nThis course is designed to provide students with the skills and knowledge they need to effectively implement MLOps and deployment practices in a business or industry setting. Students will learn about different types of databases and how to use them in ML projects, techniques for processing large datasets using Big Data workflows, and how to build and deploy APIs for machine learning models. These skills are essential for businesses that rely on machine learning to improve their products or services, such as a retail company using machine learning to personalize customer recommendations or a healthcare organization using machine learning to improve patient diagnosis and treatment.\nStudents will also learn about using modern tooling for experiment tracking, packaging and deploying machine learning models, and setting up pipelines for continuous integration and deployment. These tools are critical for ensuring the quality and reliability of machine learning models in production environments. By the end of the course, students will be able to confidently apply these techniques to improve the speed and reliability of their machine learning projects.\nCourse lectures will be complemented by a number of guest presentations with industry examples how MLOps technologies are used in different businesses.\nSession Overview Databases in ML projects Session 1: Introduction to databases for ML. Overview of different database types.\nSession 2: Working with SQL databases.\nSession 3: Working with NoSQL databases, such as TinyDB.\nExercise 1: Building PoC with DB backend + Group Portfolio Assignment\nBig Data workflows Session 4: Introduction to Big Data workflows using Spark.\nIndustry case 1: Guest lecture TBC.\nExercise 2: Performing a Big Data workflow with Spark + Group Portfolio Assignment\nFrom notebook to API Session 5: Code refactoring for production.\nSession 6: API workflows with FastAPI.\nIndustry case 2: Guest lecture TBC.\nExercise 3: API workflows + Group Portfolio Assignment\nMLOps with mlflow Session 7: Introduction to MLOps with mlflow.\nSession 8: Model deployment with mlflow.\nIndustry case 2: Guest lecture TBC.\nExercise 4: MLOps workflows + Group Portfolio Assignment\nPackaging and deployment Session 9: Introduction to packaging and deployment.\nSession 10: Introduction to Docker and deploying scalable ML.\nIndustry case 3: Guest lecture TBC.\nExercise 5: Packaging and deployment + Group Portfolio Assignment\nLiterature John, M. M., Olsson, H. H., \u0026amp; Bosch, J. (2021, September). Towards mlops: A framework and maturity model. In 2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA) (pp. 1-8). IEEE\nCalefato, F., Lanubile, F., \u0026amp; Quaranta, L. (2022, September). A preliminary investigation of MLOps practices in GitHub. In Proceedings of the 16th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (pp. 283-288).\nM√§kinen, S., Skogstr√∂m, H., Laaksonen, E., \u0026amp; Mikkonen, T. (2021, May). Who needs MLOps: What data scientists seek to accomplish and how can MLOps help?. In 2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN) (pp. 109-112). IEEE\nOpenja, M., Majidi, F., Khomh, F., Chembakottu, B., \u0026amp; Li, H. (2022). Studying the Practices of Deploying Machine Learning Projects on Docker. arXiv preprint arXiv:2206.00699.\nGranlund, T., Kopponen, A., Stirbu, V., Myllyaho, L., \u0026amp; Mikkonen, T. (2021, May). Mlops challenges in multi-organization setup: Experiences from two real-world cases. In 2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN) (pp. 82-88). IEEE\nHow Netflix works: the (hugely simplified) complex stuff that happens every time you hit Play (2017)\nBowles, M. (2019). Machine Learning with Spark and Python: Essential Techniques for Predictive Analytics. John Wiley \u0026amp; Sons.\nMade with ML (2023)\nSQLite vs TinyDB (2021)\nHow to Handle Large Datasets in Python (2022)\nWhat is MLOps (2019)\nMLOps Best Practices for Machine Learning Model Development, Deployment, and Maintenance (2022)\nBuild and Run a Docker Container for your Machine Learning Model (2021)\n[Step-by-Step MLflow Implementations](https://medium\n"},{"uri":"https://aaubs.github.io/ds23/en/m6/05_deployment/","title":"Deployment","tags":[],"description":"","content":"In this part, we will cover some technologies and approaches related to deployment.\nTechnologies include: FastAPI (repetition) Docker CI/CD pipelines (github actions / using serverless systems like GCP Cloud Run) "},{"uri":"https://aaubs.github.io/ds23/en/m1/01_basics/05-eda-hackathon/","title":"M1 EDA Hackathon ","tags":[],"description":"","content":" Team of corgis winning a hackathon. 2022. Roman x Stable Diffusion\nIn this hackathon, you will be using everything you have learned so far (and some things still coming up next week) to create a striking and informative dashboard. The winning group will receive an even more amazing prize!\nWhat to do? In groups, find an exciting dataset of similar complexity as the ones we worked with in class (e.g. comparable to Open Policing, Airbnb). You can find them, for instance, on Kaggle.\nConsider also:\nhttps://github.com/shreyashankar/datasets-for-good https://www.tableau.com/learn/articles/free-public-data-sets Steps for the analysis\nPerform EDA and answer interesting questions about the data. You can use geospatial analysis (if you like). You do not have to scrape or generate data, but you can. Perform analyses first in a notebook and generate all relevant calculations and plots there. On Sept. 13th, we\u0026rsquo;ll introduce you to Streamlit, a framework for webapp development. You are obviously welcome to look into it on your own before that. The submission deadline will be on the 15th (noon) - as a deployed (accessible online) web app. You will pitch your dashboard in class. We will forward all submitted apps to an expert jury for evaluation. The winning team will be announced in the week between 19. and 23. September. üéâ\nThe Jury Mathias Boe Flinta CTO at Scandinavian Medical Solutions\nMathias is Head of Data \u0026amp; Analytics \u0026amp; IT at SMS (Scandinavian Medical Solutions). He finished his studies in cand.oecon. (MSc. Economics) one year ago, during which he specialized in BI and Data Science. Currently, he is implementing a new ERP system, which will set a good foundation for BI analysis and predictive foresting with Data Science within the coming year.\nKarolina Grodzinska Data Analyst Co-op @ Schneider Electric, Boston US\nKarolina currently works as a data analyst at Schneider Electric. She\u0026rsquo;s also been chosen as a Tableau Student Ambassador for the upcoming academic year. In her free time, she likes to participate in data visualization challenges.\nDavid Jan Lazar Data Scientist at DataSentics, an Atos Company.\nDavid has completed a Social Data Science semester at Aalborg University and a Masters Degree from Business Intelligence at Aarhus University. He works as a data scientist at DataSentics, a consulting firm that provides business solutions based on machine learning in areas of business, finance, insurance, natural language processing, computer vision and more.\n"},{"uri":"https://aaubs.github.io/ds23/en/info/05_requirements_project/","title":"Semester Project Requirements","tags":[],"description":"","content":"Format Functional and self-contained notebook Happy to see GitHub repos (which you can use as your portfolio in the job market) Project report (30-ish pages - max. 45) Some study relation (but that is debatable and not necessarily required) Report is a (semi/non) technical documentation. Think about a corporate censor that you try to inform Content Problem formulation with some practical and theoretical motivation (no huge literature discussion) Methodology (not a critical realist vs positivist discussion but some ideas about what can be concluded potentially) Data sourcing and pre-processing strategy Overall architecture of the model(s) Modelling (incl. finetuning) Results Discussion / Conclusion Scope Uses different methods from the course (at least 2 modules) in a creative way Downloading data from kaggle/github and running an ML model is probably not enough for a good performance Creative combinations of methodologies, please: combine financial data with social media data to look at equity development extract information from text data and create networks. Use network indicators to supplement company data Evaluation will focus on correct application and communication of DS methods The level of \u0026ldquo;technicality\u0026rdquo; is as in the course with emphasis on application and intuition, not on ML engineering / mathematics However, you will need to demonstrate insight into statistics on a level that is required to discuss your assignment e.g. interpret and discuss performance indicators, outline strategies for improvement e.g. under/oversampling "},{"uri":"https://aaubs.github.io/ds23/en/m1/01_basics/06-rapid-prototyping-stramlit/","title":"Rapid Prototyping with Streamlit","tags":[],"description":"","content":"\nStreamlit was first released in October 2019 and has gained enormous popularity in the past year. The reason behind the framework\u0026rsquo;s success is the ease with which it allows data scientists to build data-driven web apps without the need to deal with frontend development or other dev-ops stuff while allowing them to incorporate all kinds of functions going far beyond just dashboards. Going from a Jupyter Notebook to a Streamlit app just requires adding a few lines of code and rewriting a few minor things.\nFrom ipynb to web-app In this tutorial, we will be going back to this Airbnb EDA Notebook from last week and building a little web-app from it.\nPlan of attack\nWe will isolate the data-prep and visualization parts we are interested in Build \u0026amp; test the app in uCloud Deploy the app via GitHub to the Streamlit Cloud (this step is optional, as you can also deploy via uCloud - ~2kr/day server costs) uCloud is a uninversity cloud service. You get 1000 DKK and 50GB storage to start with but you can apply for more. This is usually granted, as the service is not used a lot. It\u0026rsquo;s a great place to learn about modern platforms, infrastructure and more. You can play with different types of installations in a safe environment. You can also request very powerful machines.\nUCloud Set-up For this project you will need 2 app-containers running: Coder-python and Streamlit. Both can run with minimal CPU/RAM requirements. Streamlit can only run once you created a project in Code-python and saved an app.py file. It is a good idea, to create a public link and connect it to the streamlit-app. Thus, you can try out your app on your phone or share it. Saving changes in app.py will trigger imediate recomplies and your app will update everywhere.\nStreamlit syntax and layout Now, what do we need to turn our notebook into a web app?\n# 1. page-config st.set_page_config(page_title=\u0026#39;Streamlit - Dashboard ü§Ø\u0026#39;, page_icon=\u0026#34;üöÄ\u0026#34;, layout=\u0026#39;wide\u0026#39; ) # 2. Page layout - e.g. a title st.title(\u0026#34;AirBnb rentals in Copenhagen üá©üá∞\u0026#34;) Streamlit layout follows your script - things that come first, will be displayed first\u0026hellip;etc.\nLoading and preprocessing the data We can just proceed as in a notebook, but it is useful to rewrite the data loading and preprocessing into a function and add the @st.experimental_singleton decorator. Streamlit performs a re-run every time something is chaged (UI) by the user e.g. a new filter is set. To reduce processing time it\u0026rsquo;s a good idea not to re-run data-loading every single time.\n# LOAD DATA ONLY ONCE @st.experimental_singleton def load_data(): data = pd.read_csv(\u0026#39;http://data.insideairbnb.com/denmark/hovedstaden/copenhagen/2022-06-24/visualisations/listings.csv\u0026#39;) # also preprocess as we did in the notebook data = data[data.number_of_reviews \u0026gt; 0] data = data[data.room_type.isin([\u0026#39;Private room\u0026#39;, \u0026#39;Entire home/apt\u0026#39;])] data[\u0026#39;price_z\u0026#39;] = (data[\u0026#39;price\u0026#39;] - data[\u0026#39;price\u0026#39;].mean())/data[\u0026#39;price\u0026#39;].std(ddof=0) data[\u0026#39;price_z\u0026#39;] = data[\u0026#39;price_z\u0026#39;].abs() data = data[data.price_z \u0026lt; 3] data[\u0026#39;log_price\u0026#39;] = np.log(data[\u0026#39;price\u0026#39;]) return data # LOAD THE DATA NOW! data = load_data() The plots to be rendered We will go for 2 plots. A geo-visualization using pydeck and a simple altair bar plot to show prices in different areas of town.\nGeoplot\nlayer = pdk.Layer( \u0026#34;ScatterplotLayer\u0026#34;, data=data[[\u0026#39;name\u0026#39;,\u0026#39;room_type\u0026#39;,\u0026#39;price\u0026#39;, \u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;]].dropna(), pickable=True, opacity=0.7, stroked=True, filled=True, radius_scale=10, radius_min_pixels=1, radius_max_pixels=100, line_width_min_pixels=1, get_position=[\u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;], get_radius=10*\u0026#34;log_price\u0026#34;, get_color=[255, 140, 0], get_line_color=[0, 0, 0], ) # Set the viewport location view_state = pdk.ViewState(latitude=data[\u0026#39;latitude\u0026#39;].mean(), longitude=data[\u0026#39;longitude\u0026#39;].mean(), zoom=12, pitch=50) # Renders r = pdk.Deck(layers=[layer], initial_view_state=view_state, #map_style=\u0026#39;mapbox://styles/mapbox/light-v9\u0026#39;, tooltip={\u0026#34;text\u0026#34;: \u0026#34;{name}\\n{room_type}\\n{price}\u0026#34;} ) Altair barplot\nWhen using altair, we need to add one more thing, which is the number of processed observations. Altair doesn\u0026rsquo;t want to process beyond 5000 observations. That means that you have two options. Either you limit your data, as we will do here, or you pre-computed things in e.g. pandas. Here we will use 2 simple if-statements to make sure that data is always max 5000 observations.\nif len(data) \u0026gt; 5000: data_alt = data.sample(5000) if len(data) \u0026lt;= 5000: data_alt = data We use altair to create a bar chart and let it calculate the mean of the price variable (x-axis) with y and colours being split by the room type. We spread the chart across rows that represent neighbourhoods. Also, we add a tooltip that displays the values for the individual bars. We set strokeWidth to 0 to make things a bit tidier.\nprice_chart = alt.Chart(data).mark_bar().encode( x=\u0026#39;mean(price):Q\u0026#39;, y=alt.Y(\u0026#39;room_type:O\u0026#39;,axis=alt.Axis(labels=False), title=\u0026#34; \u0026#34;), color=alt.Color(\u0026#39;room_type:N\u0026#39;, scale=alt.Scale(scheme=\u0026#39;lightorange\u0026#39;)), row=\u0026#39;neighbourhood:N\u0026#39;, tooltip=[\u0026#34;neighbourhood:N\u0026#34;, \u0026#34;mean(price):Q\u0026#34;] ).configure_view(strokeWidth=0).interactive() Introductin UI / Filters We\u0026rsquo;ll introduce 2 filters in the main page (you could also move them to the sidebar): price range and neighbourhood. As you can see below, the st.sliderand st.multiselect produce python objects (tuple and list) that we can use to filter our data DataFrame with plain Pandas. e.g. When only 3 neighbourhoods are selected neighbourhood_select will turn a list of 3 elements and data[data.neighbourhood.isin(neighbourhood_select)] will result in a dataframe where only listings from these areas are present.\n#filter for price-range price_selected = st.slider(\u0026#34;Select price range\u0026#34;, min_value = int(data.price.min()), max_value= int(data.price.max()), value = (300,3000), step=50) data = data[(data.price \u0026gt; price_selected[0]) \u0026amp; (data.price \u0026lt; price_selected[1])] #filter for neighborhoods neighbourhood_select = st.multiselect(\u0026#39;Select neighbourhoods\u0026#39;, data.neighbourhood.unique(), data.neighbourhood.unique()) data = data[data.neighbourhood.isin(neighbourhood_select)] Rendering the visualizations So far nothing is displayed. We only created the objects r(pydeck map) and price_chart altair chart. It\u0026rsquo;s up to you how you like to handle it. I find it a bit easier to separate compute and render parts. Here we are going to add a horizontal column split (to make things a bit more pretty). We split the screen into 5 parts, where the first gets 3 and the second 2. To display our chats, we use the streamlit functions st.pydeck_chart and st.altair_chart. You will find many more options in the streamlit documentation.\nrow1_1, row1_2 = st.columns((3, 2)) with row1_1: st.pydeck_chart(r) with row1_2: st.altair_chart(price_chart, use_container_width=False) Requirements Our imports for that project look like this:\nimport streamlit as st import streamlit.components.v1 as components import pydeck as pdk import numpy as np import pandas as pd import altair as alt alt.renderers.set_embed_options(theme=\u0026#39;dark\u0026#39;) In contrast to a notebook, we cannot really install packages on the fly. Considering a simple python deployment environment, we need to specify what packages (aside from streamlit) need to be installed so that the app can run. This is often done by adding a requirements.txt file to the project folder. We add all libraries that we loaded just to be sure. This is not always the best idea, as things can clash\u0026hellip; but that is a chapter of its own to be covered at a later point. üòµ\npydeck altair numpy pandas All code in one place requirements.txt Expand to see code... pydeck numpy pandas altair app.py Expand to see code... #imports import streamlit as st import streamlit.components.v1 as components import pydeck as pdk import numpy as np import pandas as pd import altair as alt alt.renderers.set_embed_options(theme=\u0026#39;dark\u0026#39;) # page config st.set_page_config(page_title=\u0026#39;Streamlit - Dashboard ü§Ø\u0026#39;, page_icon=\u0026#34;üöÄ\u0026#34;, layout=\u0026#39;wide\u0026#39; ) #load data @st.experimental_singleton def load_data(): data = pd.read_csv(\u0026#39;http://data.insideairbnb.com/denmark/hovedstaden/copenhagen/2022-06-24/visualisations/listings.csv\u0026#39;) data = data[data.number_of_reviews \u0026gt; 0] data = data[data.room_type.isin([\u0026#39;Private room\u0026#39;, \u0026#39;Entire home/apt\u0026#39;])] data[\u0026#39;price_z\u0026#39;] = (data[\u0026#39;price\u0026#39;] -data[\u0026#39;price\u0026#39;].mean())/data[\u0026#39;price\u0026#39;].std(ddof=0) data[\u0026#39;price_z\u0026#39;] = data[\u0026#39;price_z\u0026#39;].abs() data = data[data.price_z \u0026lt; 3] return data data = load_data() # 2. Page layout - e.g. a title st.title(\u0026#34;AirBnb rentals in Copenhagen üá©üá∞\u0026#34;) #filter for price-range price_selected = st.slider(\u0026#34;Select price range\u0026#34;, min_value = int(data.price.min()), max_value= int(data.price.max()), value = (300,3000), step=50) data = data[(data.price \u0026gt; price_selected[0]) \u0026amp; (data.price \u0026lt; price_selected[1])] #filter for neighborhoods neighbourhood_select = st.multiselect(\u0026#39;Select neighbourhoods\u0026#39;, data.neighbourhood.unique(), data.neighbourhood.unique()) data = data[data.neighbourhood.isin(neighbourhood_select)] #geoplot layer = pdk.Layer( \u0026#34;ScatterplotLayer\u0026#34;, data=data[[\u0026#39;name\u0026#39;,\u0026#39;room_type\u0026#39;,\u0026#39;price\u0026#39;, \u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;]].dropna(), pickable=True, opacity=0.7, stroked=True, filled=True, radius_scale=10, radius_min_pixels=1, radius_max_pixels=100, line_width_min_pixels=1, get_position=[\u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;], get_radius=10*\u0026#34;log_price\u0026#34;, get_color=[255, 140, 0], get_line_color=[0, 0, 0], ) # Set the viewport location view_state = pdk.ViewState(latitude=data[\u0026#39;latitude\u0026#39;].mean(), longitude=data[\u0026#39;longitude\u0026#39;].mean(), zoom=12, pitch=50) # Renders r = pdk.Deck(layers=[layer], initial_view_state=view_state, #map_style=\u0026#39;mapbox://styles/mapbox/light-v9\u0026#39;, tooltip={\u0026#34;text\u0026#34;: \u0026#34;{name}\\n{room_type}\\n{price}\u0026#34;} ) # prefilter for altair if len(data) \u0026gt; 5000: data_alt = data.sample(5000) if len(data) \u0026lt;= 5000: data_alt = data #altair plot price_chart = alt.Chart(data).mark_bar().encode( x=\u0026#39;mean(price):Q\u0026#39;, y=alt.Y(\u0026#39;room_type:O\u0026#39;,axis=alt.Axis(labels=False), title=\u0026#34; \u0026#34;), color=alt.Color(\u0026#39;room_type:N\u0026#39;, scale=alt.Scale(scheme=\u0026#39;lightorange\u0026#39;)), row=\u0026#39;neighbourhood:N\u0026#39;, tooltip=[\u0026#34;neighbourhood:N\u0026#34;, \u0026#34;mean(price):Q\u0026#34;] ).configure_view(strokeWidth=0).interactive() # display and layout row1_1, row1_2 = st.columns((3, 2)) with row1_1: st.pydeck_chart(r) with row1_2: st.altair_chart(price_chart, use_container_width=False) "},{"uri":"https://aaubs.github.io/ds23/en/m1/01_basics/07-math/","title":"Mathematics (Brushup)","tags":[],"description":"","content":"How does Math help us? Machines can only understand numbers. For instance, to perform ML algorithms on text and multimedia, they need to be stored in vectors, matrices, and tensors. An example would be the representation of text as a vector of 768 dimensions. Using linear algebra, you can manipulate vectors, matrices, and tensors.\nWe will focus on understanding the mathematics behind ML algorithms in two dimensions. This tutorial will provide you with an overview of how machine learning algorithms work when applied to higher-dimensional data.\nPart 1: Vectors Notebook: Vectors - Definition, Properties, Types, and Applications Notebook: Vectors - Exercises Notebook: Vectors - Exercises and Solutions Part 2: Matrices Notebook: Matrices - Definition, Properties, Types, and Applications Notebook: Matrices - Exercises Notebook: Matrices - Exercises and Solutions Part 3: Optimization and Regularization Notebook: Optimization and Regularization "},{"uri":"https://aaubs.github.io/ds23/en/m6/01_databases/09_assignment_db/","title":"Portfolio Assignment Databases","tags":[],"description":"","content":"Portfolio Exercise Databases: Introduction Now it is time to get some coding done again :) This will be our first portfolio exercise,where you can practice your new skills. Task Develop a Proof-of-Concept version of an application that is querying a database to come provide an output to the user. This can be for example: Selecting observations from database, performing prediction with a (beforehand fitted) SML model. Perform a UML procedure on observations queried from a database. Perform a semantic/similarity search for an user input, retrieve most similar docs from a database. The data used should be non-trivial (eg.: enough observations,¬¥maybe multiple tables, different types of data\u0026hellip;) The solution has to be self-contained. This can be done: Within a colab using for grad.io. (Hint: An option is to save the database on github, and then load it in the colab).) As a streamlit app (figure out how to make it self-contained). \u0026hellip; (sky is the limit.) Possible databases:\nSQL DB (eg. SQL-lite) NoSQL DB Document (eg. tinyDB) Vector (Eg. Faiss, Chroma) Delivery Create a github repository (or use the existing one and adapt it) Provide a readme.md with brief description. Submission can be in groups up to 3. Deadline 24/3. Submit by sending an email with link to repo to Hamid (hamidb@business.aau.dk) with Daniel \u0026amp; Roman in cc. (dsh@\u0026hellip;, roman@\u0026hellip;) "},{"uri":"https://aaubs.github.io/ds23/en/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://aaubs.github.io/ds23/en/","title":"Social / Business Data Science 2022","tags":[],"description":"","content":"Social \u0026amp; Business Data Science 2022 Aalborg University Business School The corresponding Aalborg University Moodle course page can be found here. Note that for updated content this page rather than Moodle will be used. At AAUBS Data Science we believe in the power of open science and open education. Following AAU‚Äôs ‚ÄúKnowledge for the world‚Äù strategy, we aim at making our material available outside password protected university systems.\n"},{"uri":"https://aaubs.github.io/ds23/en/tags/","title":"Tags","tags":[],"description":"","content":""}]