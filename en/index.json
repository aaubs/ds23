[{"uri":"https://aaubs.github.io/ds23/en/m1/01_intro_ds/01_welcome_ds/","title":"- Welcome Students!","tags":[],"description":"","content":"\nThis session will introduce you to the fundamentals of data science, with a focus on Python. We will cover the Python data science stack, essential tools and platforms, software setup, semester overview, and Python 101.\nSession 1: Welcome Students! This session sets the stage for your data science journey:\nPython Data Science Stack: Dive into Python\u0026rsquo;s core data science libraries and frameworks. We\u0026rsquo;ve got you covered!\nEcosystem Deep Dive: Familiarize yourself with essential tools and platforms, such as Github, UCloud, Google Colab, and Jupyter. These will be integral to your studies and projects.\nSoftware Setup: We\u0026rsquo;ll guide you through installing the crucial software. And don\u0026rsquo;t worry, our Teaching Assistants are here to assist with any challenges.\nSemester Overview: Get a glimpse of what the upcoming weeks hold for you.\nPython 101: We\u0026rsquo;ll ensure everyone is up to speed with Python basics.\nNotebooks How to build a development environment using Colab, Google Drive, GitHub, and Kaggle!\nüöÄ Notebook: Python 101\n"},{"uri":"https://aaubs.github.io/ds23/en/info/","title":"Info, Schedule &amp; Co","tags":[],"description":"","content":"General info about the semester will be updated here. Check out the calendar on moodle.\nIntro to the semester and module: "},{"uri":"https://aaubs.github.io/ds23/en/m1/02_rapid_prototyping/01_rapid_prototyping/","title":"Introduction to Streamlit: Building an Employee Attrition Dashboard","tags":[],"description":"","content":" Corgi working on a Data Science project. 2023. Roman x Stable Diffusion XL\nStreamlit has rapidly become a go-to tool for data scientists and developers wanting to turn data scripts into shareable web apps. Let\u0026rsquo;s explore its core features and benefits:\nKey Features of Streamlit Simplicity: With just a few lines of Python code, you can have a running web application. No need to deal with HTML, CSS, or JavaScript unless you want to. Interactive Widgets: Streamlit offers out-of-the-box widgets like sliders, buttons, and text inputs that make your app interactive. Data Integration: It seamlessly integrates with popular data science libraries like Pandas, Numpy, Matplotlib, and others. Data Caching: With @st.cache, Streamlit caches the output of functions, ensuring your data operations are efficient and your apps remain performant. Hot Reloading: As you save changes to your script, the app refreshes in real-time. No need to manually restart your app. vs. Gradio While Streamlit offers a robust platform for creating web apps, Gradio provides several distinct advantages, especially when the focus is on deploying machine learning models:\nEase of Model Deployment: Gradio prioritizes simplifying the deployment of machine learning models. Its intuitive Python API allows for quick interface generation, ensuring accessibility even for non-experts. Diverse Input Types: Gradio supports a wide variety of input formats, from images and text to audio, providing flexibility especially when dealing with different data types. Multi-Model Deployment: Gradio stands out with its capability to simultaneously deploy multiple models, perfect for ensemble methods or side-by-side model comparisons. Shareability: Gradio\u0026rsquo;s feature of generating shareable URLs makes collaboration and showcasing a breeze. Security Features: Gradio offers built-in adversarial robustness, adding an extra layer of protection against potential adversarial attacks on deployed models. These unique features make Gradio an attractive option for projects that focus on deploying and sharing machine learning models with diverse requirements. We are going to explore gradio after introductions to ML.\nWhy Choose Streamlit? Rapid Prototyping: Streamlit\u0026rsquo;s intuitive API and hot reloading mean you can quickly iterate and refine your app. Open Source: Being open-source, it boasts a strong community that contributes to its growth and offers a plethora of community plugins. Deployment Ready: With platforms like Streamlit Sharing, deploying your app to the world is just a click away. Extensible: You can integrate advanced JavaScript features or even other Python libraries to extend Streamlit\u0026rsquo;s capabilities. Core Components of Streamlit Layout and Widgets Layout: st.columns and st.container can be used to design your app\u0026rsquo;s layout. Widgets: These are interactive elements like st.slider(), st.selectbox(), and st.button() that capture user input. Display Elements Media: Display images, videos, or audio clips using st.image(), st.video(), and st.audio(). Charts: Use st.line_chart(), st.bar_chart(), or integrate with libraries like Altair for custom visualizations. Tables: Showcase data with st.table() or st.dataframe(). Session State State: Store user data or app state across reruns with st.session_state. Dive Deeper To truly master Streamlit, it\u0026rsquo;s recommended to experiment with building various apps and exploring its official documentation. The community is active, and there\u0026rsquo;s always something new to learn!\nWhat You\u0026rsquo;ll Build By the end of this tutorial, you\u0026rsquo;ll have a functional dashboard that allows users to:\nFilter data based on various employee metrics. Visualize attrition rates and patterns across different dimensions. Understand insights from visualizations. Get actionable recommendations based on data insights. Prerequisites Before you begin, ensure you have the following:\nBasic knowledge of Python. Streamlit installed (pip install streamlit). Familiarity with data visualization libraries like altair, matplotlib, and seaborn. Getting Started 1. Data Source We\u0026rsquo;ll use a synthetic dataset on employee attrition from this link.\n# Import necessary libraries import streamlit as st import pandas as pd import altair as alt import matplotlib.pyplot as plt import seaborn as sns # Function to load the dataset @st.cache_data # Cache the function to enhance performance def load_data(): # Define the file path file_path = \u0026#39;https://raw.githubusercontent.com/aaubs/ds-master/main/apps/M1-attrition-streamlit/HR-Employee-Attrition-synth.csv\u0026#39; # Load the CSV file into a pandas dataframe df = pd.read_csv(file_path) # Create age groups and add as a new column bin_edges = [18, 25, 35, 45, 60] bin_labels = [\u0026#39;18-24\u0026#39;, \u0026#39;25-34\u0026#39;, \u0026#39;35-44\u0026#39;, \u0026#39;45-60\u0026#39;] df[\u0026#39;AgeGroup\u0026#39;] = pd.cut(df[\u0026#39;Age\u0026#39;], bins=bin_edges, labels=bin_labels, right=False) return df # Load the data using the defined function df = load_data() # Set the app title and sidebar header st.title(\u0026#34;Employee Attrition Dashboard üòäüìà\u0026#34;) st.sidebar.header(\u0026#34;Filters üìä\u0026#34;) # Introduction # HR Attrition Dashboard st.markdown(\u0026#34;\u0026#34;\u0026#34; Welcome to the HR Attrition Dashboard. In the backdrop of rising employee turnovers, HR departments are stressing the significance of predicting and understanding employee departures. Through the lens of data analytics, this dashboard unveils the deeper causes of employee churn and proposes strategies to boost employee retention. \u0026#34;\u0026#34;\u0026#34;) with st.expander(\u0026#34;üìä **Objective**\u0026#34;): st.markdown(\u0026#34;\u0026#34;\u0026#34; At the heart of this dashboard is the mission to visually decode data, equipping HR experts with insights to tackle these queries: - Which company factions face a greater likelihood of employee exits? - What might be pushing these individuals to part ways? - Observing the discerned trends, what incentives might hold the key to decreasing the attrition rate? \u0026#34;\u0026#34;\u0026#34; ) # Tutorial Expander with st.expander(\u0026#34;How to Use the Dashboard üìö\u0026#34;): st.markdown(\u0026#34;\u0026#34;\u0026#34; 1. **Filter Data** - Use the sidebar filters to narrow down specific data sets. 2. **Visualize Data** - From the dropdown, select a visualization type to view patterns. 3. **Insights \u0026amp; Recommendations** - Scroll down to see insights derived from the visualizations and actionable recommendations. \u0026#34;\u0026#34;\u0026#34;) # Sidebar filter: Age Group selected_age_group = st.sidebar.multiselect(\u0026#34;Select Age Groups üï∞Ô∏è\u0026#34;, df[\u0026#39;AgeGroup\u0026#39;].unique().tolist(), default=df[\u0026#39;AgeGroup\u0026#39;].unique().tolist()) if not selected_age_group: st.warning(\u0026#34;Please select an age group from the sidebar ‚ö†Ô∏è\u0026#34;) st.stop() filtered_df = df[df[\u0026#39;AgeGroup\u0026#39;].isin(selected_age_group)] # Sidebar filter: Department departments = df[\u0026#39;Department\u0026#39;].unique().tolist() selected_department = st.sidebar.multiselect(\u0026#34;Select Departments üè¢\u0026#34;, departments, default=departments) if not selected_department: st.warning(\u0026#34;Please select a department from the sidebar ‚ö†Ô∏è\u0026#34;) st.stop() filtered_df = filtered_df[filtered_df[\u0026#39;Department\u0026#39;].isin(selected_department)] # Sidebar filter: Monthly Income Range min_income = int(df[\u0026#39;MonthlyIncome\u0026#39;].min()) max_income = int(df[\u0026#39;MonthlyIncome\u0026#39;].max()) income_range = st.sidebar.slider(\u0026#34;Select Monthly Income Range üí∞\u0026#34;, min_income, max_income, (min_income, max_income)) filtered_df = filtered_df[(filtered_df[\u0026#39;MonthlyIncome\u0026#39;] \u0026gt;= income_range[0]) \u0026amp; (filtered_df[\u0026#39;MonthlyIncome\u0026#39;] \u0026lt;= income_range[1])] # Sidebar filter: Job Satisfaction Level satisfaction_levels = sorted(df[\u0026#39;JobSatisfaction\u0026#39;].unique().tolist()) selected_satisfaction = st.sidebar.multiselect(\u0026#34;Select Job Satisfaction Levels üòä\u0026#34;, satisfaction_levels, default=satisfaction_levels) if not selected_satisfaction: st.warning(\u0026#34;Please select a job satisfaction level from the sidebar ‚ö†Ô∏è\u0026#34;) st.stop() filtered_df = filtered_df[filtered_df[\u0026#39;JobSatisfaction\u0026#39;].isin(selected_satisfaction)] # Displaying the Attrition Analysis header st.header(\u0026#34;Attrition Analysis üìä\u0026#34;) # Dropdown to select the type of visualization visualization_option = st.selectbox( \u0026#34;Select Visualization üé®\u0026#34;, [\u0026#34;Attrition by Age Group\u0026#34;, \u0026#34;KDE Plot: Distance from Home by Attrition\u0026#34;, \u0026#34;Attrition by Job Role\u0026#34;, \u0026#34;Attrition Distribution by Gender\u0026#34;, \u0026#34;MonthlyRate and DailyRate by JobLevel\u0026#34;] ) # Visualizations based on user selection if visualization_option == \u0026#34;Attrition by Age Group\u0026#34;: # Bar chart for attrition by age group chart = alt.Chart(filtered_df).mark_bar().encode( x=\u0026#39;AgeGroup\u0026#39;, y=\u0026#39;count()\u0026#39;, color=\u0026#39;Attrition\u0026#39; ).properties( title=\u0026#39;Attrition Rate by Age Group\u0026#39; ) st.altair_chart(chart, use_container_width=True) elif visualization_option == \u0026#34;KDE Plot: Distance from Home by Attrition\u0026#34;: # KDE plot for Distance from Home based on Attrition plt.figure(figsize=(10, 6)) sns.kdeplot(data=filtered_df, x=\u0026#39;DistanceFromHome\u0026#39;, hue=\u0026#39;Attrition\u0026#39;, fill=True, palette=\u0026#39;Set2\u0026#39;) plt.xlabel(\u0026#39;Distance From Home\u0026#39;) plt.ylabel(\u0026#39;Density\u0026#39;) plt.title(\u0026#39;KDE Plot of Distance From Home by Attrition\u0026#39;) st.pyplot(plt) elif visualization_option == \u0026#34;Attrition by Job Role\u0026#34;: # Bar chart for attrition by job role chart = alt.Chart(filtered_df).mark_bar().encode( y=\u0026#39;JobRole\u0026#39;, x=\u0026#39;count()\u0026#39;, color=\u0026#39;Attrition\u0026#39; ).properties( title=\u0026#39;Attrition by Job Role\u0026#39; ) st.altair_chart(chart, use_container_width=True) elif visualization_option == \u0026#34;Attrition Distribution by Gender\u0026#34;: # Pie chart for attrition distribution by gender pie_chart_data = filtered_df[filtered_df[\u0026#39;Attrition\u0026#39;] == \u0026#39;Yes\u0026#39;][\u0026#39;Gender\u0026#39;].value_counts().reset_index() pie_chart_data.columns = [\u0026#39;Gender\u0026#39;, \u0026#39;count\u0026#39;] chart = alt.Chart(pie_chart_data).mark_arc().encode( theta=\u0026#39;count:Q\u0026#39;, color=\u0026#39;Gender:N\u0026#39;, tooltip=[\u0026#39;Gender\u0026#39;, \u0026#39;count\u0026#39;] ).properties( title=\u0026#39;Attrition Distribution by Gender\u0026#39;, width=300, height=300 ).project(\u0026#39;identity\u0026#39;) st.altair_chart(chart, use_container_width=True) elif visualization_option == \u0026#34;MonthlyRate and DailyRate by JobLevel\u0026#34;: # Boxplots for MonthlyRate and DailyRate by JobLevel fig, ax = plt.subplots(1, 2, figsize=(15, 7)) # MonthlyRate by JobLevel sns.boxplot(x=\u0026#34;JobLevel\u0026#34;, y=\u0026#34;MonthlyRate\u0026#34;, data=filtered_df, ax=ax[0], palette=\u0026#39;Set2\u0026#39;) ax[0].set_title(\u0026#39;MonthlyRate by JobLevel\u0026#39;) ax[0].set_xlabel(\u0026#39;Job Level\u0026#39;) ax[0].set_ylabel(\u0026#39;Monthly Rate\u0026#39;) # DailyRate by JobLevel sns.boxplot(x=\u0026#34;JobLevel\u0026#34;, y=\u0026#34;DailyRate\u0026#34;, data=filtered_df, ax=ax[1], palette=\u0026#39;Set2\u0026#39;) ax[1].set_title(\u0026#39;DailyRate by JobLevel\u0026#39;) ax[1].set_xlabel(\u0026#39;Job Level\u0026#39;) ax[1].set_ylabel(\u0026#39;Daily Rate\u0026#39;) plt.tight_layout() st.pyplot(fig) # Display dataset overview st.header(\u0026#34;Dataset Overview\u0026#34;) st.dataframe(df.describe()) # Insights from Visualization Section Expander with st.expander(\u0026#34;Insights from Visualization üß†\u0026#34;): st.markdown(\u0026#34;\u0026#34;\u0026#34; 1. **Age Groups \u0026amp; Attrition** - The \u0026#39;Attrition by Age Group\u0026#39; plot showcases which age brackets face higher attrition. 2. **Home Distance\u0026#39;s Impact** - The \u0026#39;KDE Plot: Distance from Home by Attrition\u0026#39; visualizes if being farther away influences leaving tendencies. 3. **Roles \u0026amp; Attrition** - \u0026#39;Attrition by Job Role\u0026#39; reveals which roles might be more attrition-prone. 4. **Gender \u0026amp; Attrition** - The pie chart for \u0026#39;Attrition Distribution by Gender\u0026#39; provides insights into any gender-based patterns. 5. **Earnings Patterns** - \u0026#39;MonthlyRate and DailyRate by JobLevel\u0026#39; boxplots display the compensation distribution across job levels. \u0026#34;\u0026#34;\u0026#34;) # Recommendations Expander with st.expander(\u0026#34;Recommendations for Action üåü\u0026#34;): st.markdown(\u0026#34;\u0026#34;\u0026#34; - üéÅ **Incentive Programs:** Introduce incentives tailored for groups showing higher attrition tendencies. - üè° **Remote Work Options:** Providing flexibility, especially for those living farther from the workplace, could reduce attrition. - üöÄ **Training \u0026amp; Growth:** Invest in employee development, especially in roles with higher attrition rates. - üë´ **Gender Equality:** Foster an environment that supports equal opportunities regardless of gender. - üí∏ **Compensation Review:** Regularly review and adjust compensation structures to stay competitive and retain talent. \u0026#34;\u0026#34;\u0026#34;) 2. File Structure Ensure your working directory has the following structure:\nüì¶Your_Directory ‚î£ üìúapp.py ‚îó üìúrequirements.txt ‚îó üìúdata.csv app.py will contain our Streamlit app\u0026rsquo;s code, while requirements.txt will list the necessary Python packages.\n3. Hosting the App Once you\u0026rsquo;ve built the app, we\u0026rsquo;ll first host it on Streamlit Cloud and then on uCloud (A Danish private cloud for universities).\nBuilding the Dashboard Here\u0026amp;rsquo;s the code to build the dashboard. Go through it, and let\u0026rsquo;s break it down.\nInitialization We start by importing necessary libraries and loading our dataset. The data is cached using @st.cache_data to enhance performance.\nDesigning the Interface Streamlit provides intuitive functions to design the user interface:\nst.title() and st.header() set titles and headers. st.markdown() allows for rich text formatting. st.sidebar lets you add interactive widgets in a sidebar for filtering. Visualizations Depending on user input, we visualize the filtered data using various charts. For instance, altair is used for bar and pie charts, while matplotlib and seaborn provide KDE plots and boxplots.\nInsights and Recommendations Finally, expanders provide a space to share insights derived from visualizations and actionable recommendations.\nAccess the Deployed App You can access the final deployed Employee Attrition Dashboard on Streamlit using the link below:\nEmployee Attrition Dashboard\nFeel free to explore the app, interact with the various filters and visualizations, and gain insights into employee attrition patterns.\nConclusion Streamlit offers a user-friendly platform to build and deploy interactive data apps without the need for extensive web development skills. Dive in, explore the code, and customize it to create your own data-driven web applications.\nHappy Coding! üöÄ\nOld corgy from last year\u0026hellip; Corgi working on a Data Science project. 2022. Roman x Stable Diffusion\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/01_intro_ds/02_data_handeling/","title":"- Data Handeling and Manipulation","tags":[],"description":"","content":"\nThis session will introduce students to the foundational aspects of data handling in Python. Students will learn about the different types of data that are important in data science, and they will explore essential operations like arrange, group-by, filter, select, and join. By the end of this session, students should have a solid understanding of primary data manipulation techniques, setting the stage for more advanced subjects.\nSession 2: Data Handling and Manipulation I (Lecture) In this session, we\u0026rsquo;ll explore the foundational aspects of data handling. Key takeaways include:\nData Manipulation: Learn essential operations like arrange, group-by, filter, select, and join, preparing data for analysis. Notebook: Python Data Manipulation By the end, students should have a solid understanding of primary data manipulation techniques, setting the stage for more advanced subjects.\nExercises Notebook: Python DS Handbook, C.2-3 exercises\nNotebook: Pandas exercises\nNotebook: Netflix Pandas exercises\nNotebook: HR Attrition Assignment\nHere, you will find the answers to the exercises:\nNotebook: Python DS Handbook, C.2-3 exercises and solutions\nNotebook: Pandas exercises and solutions\nTutorial - Assignment Examples Notebook: EDA Assignment Example - Policing Further studies Recommended DataCamp courses Introduction to Python Intermediate Python Recommended readings Python fo Data Science Handbook (VanderPlas, 2016), Chapter 2-3 "},{"uri":"https://aaubs.github.io/ds23/en/m1/02_rapid_prototyping/02_online_dashboard/","title":"- Real World Data to Online Dashboard","tags":[],"description":"","content":"This session will focus on creating interactive online dashboards using real-world data. Students will learn how to select and clean data, create visualizations, and deploy their dashboards to the web.\nDeployment: Deployment of projects as WebApps\nPart 1: AirBnb In this notebook we will be using data from AirBnb for some basic EDA and geoplotting\nEDA and Geoviz starter EDA and Geoviz class Part 2: Kaggle In this notebook we will be learning how to work with data from Kaggle as well as exercise more simple data-viz.\nKaggle starter Kaggle class What to do now?! Replay code from the course and see if you do understand the core mechanics - you DO NOT need to remember everything. Android app market project on datacamp Course: Python DS toolbox 1 \u0026amp; Course: Python DS toolbox 2 Opendata.dk - build a map of different attractions in Aalborg based on public data. See preprocessing example - how to get data out of nested JSON - below: This is how you can preprocess the GeoCoordinates from the JSON file:\n#Load pandas import pandas as pd # Read the file from remote data = pd.read_json(\u0026#39;https://admin.opendata.dk/dataset/44ecd686-5cb5-40f2-8e3f-b5e3607a55ef/resource/eeabb0f8-1b19-4c80-b059-5ba5c4c872d2/download/guidedenmarkaalborgenjson.json\u0026#39;) # The GeoCoordinates are hiding in the Address column data[\u0026#39;Address\u0026#39;][0][\u0026#39;GeoCoordinate\u0026#39;] # You can use list comprehension to pull out GeoCoordinates (also empty values) - try out # This will allow you to filter for missing data without fancy workarounds [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # Make a new column based on that to be used for filtering out missing data data[\u0026#39;GeoCoordinate\u0026#39;] = [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # drop, where no GeoCoordinate data = data.dropna(subset=[\u0026#39;GeoCoordinate\u0026#39;]) # Pull out the values data[\u0026#39;latitude\u0026#39;] = [x[\u0026#39;Latitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] data[\u0026#39;longitude\u0026#39;] = [x[\u0026#39;Longitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] Introduction to GeoPandas Using GeoPandas to analyze geospatial data will be our focus in these notebooks.\nGeoPandas GeoPandas and Solutions GeoPandas Hands-on Project GeoPandas Exercises GeoPandas exercises GeoPandas exercises and solutions "},{"uri":"https://aaubs.github.io/ds23/en/m1/01_intro_ds/","title":"A) Introduction to Data Science (W35-36)","tags":[],"description":"","content":" Note: Group Portfolio Assignment - Exploratory Data Analysis (EDA) Deadline: Friday, 8 September 2023, 12:00 PM\nThis topic includes 5 sessions as follows:\nWelcome to Data Science! (Friday, September 1st, 10:15-14:15): This session will introduce students to the fundamentals of data science, with a focus on Python. Students will learn about the Python data science stack, essential tools and platforms, and software setup. They will also get a preview of the upcoming weeks and a refresher on Python basics. Data Handling and Manipulation I (Lecture) (Monday, September 4th, 12:30-16:15): This session will cover the foundational aspects of data handling in Python. Students will learn about the different types of data that are important in data science, and they will explore essential operations like arrange, group-by, filter, select, and join. By the end of this session, students should have a solid understanding of primary data manipulation techniques. Exploratory Data Analysis \u0026amp; Essential Statistics (Tuesday, September 5th, 08:15-12:00): This session will introduce students to exploratory data analysis (EDA) and essential statistics. Students will learn how to use EDA to uncover patterns, anomalies, and frame questions in data. They will also learn about foundational measures and techniques for data interpretation. EDA-Exercise on a Real Dataset (Tuesday, September 5th, 12:30-14:15): This session will give students the opportunity to practice the EDA techniques they learned earlier on a real dataset. Students will be able to dive into a real dataset, identify patterns and insights, and implement key EDA methods. They will also benefit from on-the-spot guidance by the teacher and TAs during the exercise. Data Visualization in Data Science (Wednesday, September 6th, 10:15-14:15): This session will teach students the importance of effective data visualization in data science. Students will explore Seaborn, a Python library for intuitive statistical graphics, and Altair, a declarative visualization library for Python. They will also have the opportunity to create impactful visualizations with real datasets through hands-on exercises. "},{"uri":"https://aaubs.github.io/ds23/en/m1/","title":"Applied Data Science and Machine Learning","tags":[],"description":"","content":"M1 - Applied Data Science and Machine Learning Note: M1 - Final Assignment Deadline: Friday, 6 October 2023, 12:00 PM\nThis module provides a condensed introduction to the ‚ÄúData Science Pipeline,‚Äù introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication. This module includes the following four main topics:\nA) Introduction to Data Science B) Rapid Prototyping C) Unsupervised Machine Learning D) Supervised Machine Learning "},{"uri":"https://aaubs.github.io/ds23/en/m1/01_intro_ds/03_data_visualization_stat/","title":"- Exploratory Data Analysis and Essential Statistics","tags":[],"description":"","content":"This session introduces students how to use EDA and some fundemental concepts of statistical methods to uncover patterns, anomalies, and frame questions in data. Students will learn foundational measures and techniques for data interpretation, and they will have the opportunity to apply EDA and statistical methods on datasets through hands-on exercises.\nDeep Dive into EDA: Uncover patterns, anomalies, and frame questions. Key Statistical Concepts: Foundational measures and techniques for data interpretation. Hands-on Analysis: Apply EDA and statistical methods on datasets. Part 1: Statistics refresher Notebook statistics refresher Part 2: Further concepts Notebook propability distributions Notebook AB testing Further studies Recommended DataCamp courses Intyroduction to statistics (no coding) Statistical Thinking in Python I Statistical Thinking in Python II Statistical Simulation in Python Recommended readings Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python ###‚Ç¨ Further resources\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/02_rapid_prototyping/","title":"B) Rapid Prototyping (W37)","tags":[],"description":"","content":" Note: Dashboard-Hackathon Submission Deadline: Friday, 15 September 2023, 12:00\nThis topic includes 3 sessions as follows:\nRapid Prototyping with Streamlit \u0026amp; Gradio (Monday, September 11th, 10:15-14:15): This session will cover the basics of rapid prototyping in data science, with a focus on the Streamlit and Gradio libraries. Students will learn how to use these libraries to create interactive web applications that can be used to explore data and test hypotheses. Real-World Data to Online Dashboard (Tuesday, September 12th, 12:30-16:15): This session will focus on creating interactive online dashboards using real-world data. Students will learn how to select and clean data, create visualizations, and deploy their dashboards to the web. Dashboard-Hackathon Kick-off (Tuesday, September 12th, 16:30-18:15): This session will kick off the Data Science Dashboard Hackathon. Students will have the opportunity to compete to create the most compelling and user-friendly interactive online dashboard using Streamlit. "},{"uri":"https://aaubs.github.io/ds23/en/info/02_modules/","title":"Modules","tags":[],"description":"","content":"For Business Data Science Students M1: Data Handling, Exploration \u0026amp; Applied Machine Learning 10 ECTS\nThis module will prove a condensed introduction to the ‚ÄúData Science Pipeline‚Äù, introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication.\nM2: Network Analysis and Natural Language Processing 5 ECTS\nFocuses on analyzing a variety of unstructured data sources. Particularly, students will learn how to explore, analyze, and visualize natural language (text) as well as relational (network) data.\nM3: Data-Driven Business Modelling and Strategy 15 ECTS Course with integrated project in which you will learn how companies plan, prepare and execute data-driven projects. In the project you will work wich a company case and build a \u0026ldquo;mini\u0026rdquo; version of the product/process.\nFor Social Data Science Students - Elective Semester M3: (SDS) Deep Learning and Artificial Intelligence for Analytics 5 ECTS\nIntroduces to the most recent developments in machine learning, which are deep learning and artificial intelligence applications. The module will provide a solid foundation for this exciting and rapidly developing field. Students will learn whether and how to apply deep learning techniques for business analytics, and acquire proficiency in new methods autonomously.\nCapstone Project Semester project utilising techniques and approaches from SDS in the context of a problem related to your main study field.\n"},{"uri":"https://aaubs.github.io/ds23/en/info/04_litetrature/","title":"Literature &amp; Resources","tags":[],"description":"","content":"While this course does not come with a list of mandatory readings, we will often refer to some central resources in R and python, which for the most part can always be accessed in a free and updated online version. We generally recommend you to use these amazing resources for problem-solving and further self-study on the topic.\nMain Literature These pieces of work can be seen as main references for data science using Python. We will frequently refer to selected chapters for further study. Documentation of the used packages, tutorials, papers, podcasts etc. will be added throughout.\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O\u0026rsquo;Reilly Media, Inc. Online available here Wilke, C. O. (2019). Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O\u0026rsquo;Reilly Media. Supplementary literature Essential Math for Data Science O\u0026rsquo;Reilly Media. Nield, T. (2022): Math refresher targeting data science relevant concepts. Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python Documentation of packages/libraries used Note: Papers, Business Cases, Videos, Tutorials, Podcasts, and Blogposts will be presented and assigned during the course.\nFurther Ressources Data Science Cloud services Notebook bases: Google Colab: Googles popular service for editing, running \u0026amp; sharing Jupyter notebooks (Only Python Kernel, but R kernel can be accessed via some tricks) Deepnote: New popular online notebook service with good integration to other services (Python, R \u0026amp; more) Kaggle: Also provides their own cloud-based service co create and run computational notebooks. Convenient, unlimited, but a bit slow (Pyhton, r ). Instance based: UCloud: New cloud infrastructure provided by AAU, AU, SDU AAU Strato: AAU CLAUDIA infratructure. Very powerful, but access needs a bit of experience with working via terminal. Community Kaggle: Crowdsourced data science challanges. Nowadays also provides a vivid community where you find datasets, notebooks for all kind of data science exercises. madewithml Tools \u0026amp; Helpers "},{"uri":"https://aaubs.github.io/ds23/en/info/03_schedule/","title":"Semester Schedule","tags":[],"description":"","content":"This will be shortly updated with additional key dates and topics for the semester. For now, please follow CalMoodle.\nGeneral appointments Introduction to Semester Project and group formation: 24.10.2023, 14:30-16:00 M1: Week 35-41 Topics W 35: Introduction \u0026amp; landing W 36: Data Manipulation, Exploratory Data Analysis (EDA) W 37: Exploratory Data Analysis (EDA) / Dashboard development / Hackathon W 38: Unsupervised Machine Learning (UML), Math for ML W 39: Supervised Machine Learning (SML) W 40: Group Assignment W 41: Exam Key Dates Data Storytelling Hackathon\nIn groups: Developing EDA Dashboard 12.09.2023 - 15.09.2023 Group assignment: 29.09.-06.10.2023 (Digital Eksamen)\nFinal exam: 10-11.09.2023\n"},{"uri":"https://aaubs.github.io/ds23/en/m1/01_intro_ds/05_data_visualization_ds/","title":"- Data Visualization in Data Science","tags":[],"description":"","content":"This session will teach you the fundamentals of data visualization in data science. You will learn the importance of effective data visualization, the principles that drive meaningful visuals, and how to use two popular Python libraries for data visualization: Seaborn and Altair.\nFoundations of Visualization: Understand the importance of effective data visualization in Data Science and the principles that drive meaningful visuals. Seaborn Mastery: Explore Seaborn, a Python library for intuitive statistical graphics. Altair Exploration: Delve into Altair, a declarative visualization library for Python. Hands-on Visualization: Harness both Seaborn and Altair to craft impactful visualizations with real datasets. Notebooks Notebook Dataviz Slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides here\n"},{"uri":"https://aaubs.github.io/ds23/en/info/05_requirements_project/","title":"Semester Project Requirements","tags":[],"description":"","content":"Format Functional and self-contained notebook Happy to see GitHub repos (which you can use as your portfolio in the job market) Project report (30-ish pages - max. 45) Some study relation (but that is debatable and not necessarily required) Report is a (semi/non) technical documentation. Think about a corporate censor that you try to inform Content Problem formulation with some practical and theoretical motivation (no huge literature discussion) Methodology (not a critical realist vs positivist discussion but some ideas about what can be concluded potentially) Data sourcing and pre-processing strategy Overall architecture of the model(s) Modelling (incl. finetuning) Results Discussion / Conclusion Scope Uses different methods from the course (at least 2 modules) in a creative way Downloading data from kaggle/github and running an ML model is probably not enough for a good performance Creative combinations of methodologies, please: combine financial data with social media data to look at equity development extract information from text data and create networks. Use network indicators to supplement company data Evaluation will focus on correct application and communication of DS methods The level of \u0026ldquo;technicality\u0026rdquo; is as in the course with emphasis on application and intuition, not on ML engineering / mathematics However, you will need to demonstrate insight into statistics on a level that is required to discuss your assignment e.g. interpret and discuss performance indicators, outline strategies for improvement e.g. under/oversampling "},{"uri":"https://aaubs.github.io/ds23/en/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://aaubs.github.io/ds23/en/","title":"Social / Business Data Science 2023","tags":[],"description":"","content":"Social \u0026amp; Business Data Science 2023 Aalborg University Business School The corresponding Aalborg University Moodle course page can be found here. Note that for updated content this page rather than Moodle will be used. At AAUBS Data Science we believe in the power of open science and open education. Following AAU‚Äôs ‚ÄúKnowledge for the world‚Äù strategy, we aim at making our material available outside password protected university systems.\n"},{"uri":"https://aaubs.github.io/ds23/en/tags/","title":"Tags","tags":[],"description":"","content":""}]