<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural Language Processing on Social / Business Data Science 2023</title><link>https://aaubs.github.io/ds23/en/m2/02_nlp/</link><description>Recent content in Natural Language Processing on Social / Business Data Science 2023</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://aaubs.github.io/ds23/en/m2/02_nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Basics of NLP</title><link>https://aaubs.github.io/ds23/en/m2/02_nlp/1-nlp-intro-sml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m2/02_nlp/1-nlp-intro-sml/</guid><description>Corgies doing NLP - Medieval Fresco. 2022. Roman x Stable Diffusion
This session introduces basic concepts of NLP. We will take a Problem-Based-Learning approach and I will introduce NLP-related concepts as we go. If you need a more theoretical intro (standalone), I&amp;rsquo;ll uploaded pre-recorded videos. We will start with a project based on
Context This assignment is based on data from this paper
Davidson, T., Warmsley, D., Macy, M., &amp;amp; Weber, I.</description></item><item><title>NLP - Other areas</title><link>https://aaubs.github.io/ds23/en/m2/02_nlp/2-nlp-topic-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m2/02_nlp/2-nlp-topic-model/</guid><description>Corgies doing NLP - Medieval Fresco. 2022. Roman x Stable Diffusion
This session will continue with statistical NLP techniques in a &amp;ldquo;less guided fashion&amp;rdquo;.
Plan for the day Finish up political tweets (dem/rep classifier) Vectorization, distance and similarity for text (maybe even Word2Vec) What research is published at AAUBS? (Scopus + Topic Models) - Hamid takes over. Scopus Topic Models Homework Finish up reseach topic modeling Turning it into an app&amp;hellip; (build &amp;amp; deploy a text classifier app - template code below) Can you beat the score?</description></item><item><title>NLP Applications Chatbot</title><link>https://aaubs.github.io/ds23/en/m2/02_nlp/3-nlp-advanced/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m2/02_nlp/3-nlp-advanced/</guid><description>Roman x Stable Diffusion
Chatbots are one of the most wide spread NLP applications. In this tutorial we will build a simple retrieval chatbot that can be used for example as an alternative for FAQ applications in companies.
The approach is following:
Train a model on variants of a question. Take input and predict the type of question asked - this is called &amp;ldquo;intent&amp;rdquo; Reply with a pre-defined response corresponding to the question asked.</description></item><item><title>NLP Word Vectors</title><link>https://aaubs.github.io/ds23/en/m2/02_nlp/4-nlp-embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m2/02_nlp/4-nlp-embeddings/</guid><description>Roman x Stable Diffusion
Word embeddings such as Word2Vec or fastText are probably the last techology in NLP before the deep learning revolution. Others would argue that they paved the way for DL becoming the NLP standard. May that as it be, it&amp;rsquo;s a powerfull approach and in this tutorial you will learn about the theory behind these word representations, how they are trained (also from disk). We will also explore approaches to representing text using pretrained vectors.</description></item></channel></rss>