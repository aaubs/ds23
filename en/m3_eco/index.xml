<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Applied Deep Learning and Artificial Intelligence on Social / Business Data Science 2023</title><link>https://aaubs.github.io/ds23/en/m3_eco/</link><description>Recent content in Applied Deep Learning and Artificial Intelligence on Social / Business Data Science 2023</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://aaubs.github.io/ds23/en/m3_eco/index.xml" rel="self" type="application/rss+xml"/><item><title>Intro to Traditional Deep Learning</title><link>https://aaubs.github.io/ds23/en/m3_eco/01_intro-to-traditional-deep-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m3_eco/01_intro-to-traditional-deep-learning/</guid><description>This session provides an overview of the foundational elements of deep learning, including its historical context, key concepts, and practical applications. The course will delve into various types of neural networks, outlining their advantages and disadvantages. It will specifically focus on convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks, highlighting their unique characteristics and applicability to a range of problem-solving scenarios, including those in economics.</description></item><item><title>Group Assignment 1</title><link>https://aaubs.github.io/ds23/en/m3_eco/01_group_assignment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m3_eco/01_group_assignment/</guid><description>Portfolio Exercise 1 Note: M3 - Group Assignment 1 Deadline: Monday, 6th of November at 12:00 PM
Introduction In this assignment, you are required to delve into the practical aspects of Deep Learning by constructing and evaluating a neural network using PyTorch. This exercise is designed to deepen your understanding of neural network architectures, hyperparameter tuning, and the preprocessing steps necessary for effective model training and evaluation. You will have the freedom to choose a dataset from either the M1 or M2 module or select an external dataset that intrigues you.</description></item><item><title>Intro to Transformer Models</title><link>https://aaubs.github.io/ds23/en/m3_eco/02_intro-tm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m3_eco/02_intro-tm/</guid><description>Literature Sutskever, I., Vinyals, O., &amp;amp;amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &amp;amp;hellip; &amp;amp;amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.
The illustrated transformer
Simple transformer LM
Notebooks - Basics Transformer Models - Basics Notebooks - Applications TM Applications - SBERT TM Applications - HF Simple transformer LM SBERT for Patent Search using PatentSBERTa in PyTorch Notebooks - FineTuning TM FineTuning - SimpleTransformers TM FineTuning - SBERT TM FineTuning - HF SetFit Hatespeech vs bert and distilroberta Seq2Seq - Neural Machine Translation Slides - Attention Mechanism Slides - SBERT Classification with various vectorization approaches TF-IDF and W2V Multi-Class Text Classification BERT Multi-Class Text Classification Implementing Multi-Class Text Classification LSTMs using PyTorch Resources OG SBERT-Paper Reimers, N.</description></item><item><title>Group assignment 2</title><link>https://aaubs.github.io/ds23/en/m3_eco/02_group_assignment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m3_eco/02_group_assignment/</guid><description>Portfolio Exercise 2: Transformer Models Note: M3 - Group Assignment 2 Deadline: Wednesday, November 15th at 10:00 AM
Introduction This exercise is designed to deepen your understanding and skills in modern deep learning techniques. We have two main tasks for you. The first is focused on using SBERT for semantic search, and the second involves hands-on exercises with gradient descent and the attention mechanism.
Part 1: SBERT and Semantic Search Task Description Create something innovative using SBERT and semantic search, or even more!</description></item><item><title>Intro to Generative Pre-trained Models</title><link>https://aaubs.github.io/ds23/en/m3_eco/02_intro-gpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m3_eco/02_intro-gpt/</guid><description>
Notebooks - Basics Generative Pre-trained Models - Basics Notebooks - Applications TM Applications - LangChain 1 TM Applications - LangChain 1 Notebooks - FineTuning Resources LangChain LanceDB - Vector database</description></item></channel></rss>