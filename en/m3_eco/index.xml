<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Applied Deep Learning and Artificial Intelligence on Social / Business Data Science 2023</title><link>https://aaubs.github.io/ds23/en/m3_eco/</link><description>Recent content in Applied Deep Learning and Artificial Intelligence on Social / Business Data Science 2023</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://aaubs.github.io/ds23/en/m3_eco/index.xml" rel="self" type="application/rss+xml"/><item><title>Intro to Traditional Deep Learning</title><link>https://aaubs.github.io/ds23/en/m3_eco/01_intro-to-traditional-deep-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m3_eco/01_intro-to-traditional-deep-learning/</guid><description>This session provides an overview of the foundational elements of deep learning, including its historical context, key concepts, and practical applications. The course will delve into various types of neural networks, outlining their advantages and disadvantages. It will specifically focus on convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks, highlighting their unique characteristics and applicability to a range of problem-solving scenarios, including those in economics.</description></item><item><title>Group Assignment 1</title><link>https://aaubs.github.io/ds23/en/m3_eco/01_group_assignment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m3_eco/01_group_assignment/</guid><description>Portfolio Exercise 1 Note: M3 - Group Assignment 1 Deadline: Monday, 6th of November at 12:00 PM
Introduction In this assignment, you are required to delve into the practical aspects of Deep Learning by constructing and evaluating a neural network using PyTorch. This exercise is designed to deepen your understanding of neural network architectures, hyperparameter tuning, and the preprocessing steps necessary for effective model training and evaluation. You will have the freedom to choose a dataset from either the M1 or M2 module or select an external dataset that intrigues you.</description></item><item><title>Introduction to Transformer Models</title><link>https://aaubs.github.io/ds23/en/m3_eco/02_intro-tm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds23/en/m3_eco/02_intro-tm/</guid><description>
Literature Sutskever, I., Vinyals, O., &amp;amp;amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &amp;amp;hellip; &amp;amp;amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.
The illustrated transformer
Simple transformer LM
Notebooks - Basics Transformer Models - Basics Notebooks - Applications TM Applications - SBERT TM Applications - HF Seq2Seq - Neural Machine Translation Simple transformer LM Notebooks - FineTuning Slides</description></item></channel></rss>