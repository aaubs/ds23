<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Rransformer models in 2023 on Social / Business Data Science 2022</title><link>https://aaubs.github.io/ds22/m4/04_transformer/</link><description>Recent content in Rransformer models in 2023 on Social / Business Data Science 2022</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://aaubs.github.io/ds22/m4/04_transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>Fine-tuning transformers</title><link>https://aaubs.github.io/ds22/m4/04_transformer/1_basictm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aaubs.github.io/ds22/m4/04_transformer/1_basictm/</guid><description>Resources You will need a HFü§ó account to upload models We will use WandB for monitoring - get an account if you like to follow that part Notebooks Transformer-based Translation (inference)
Finetuning Sequence Classification with Bert
Homework - As a preparation for final assignment‚ÄºÔ∏è Finetune a model for token-classification Follow this tutorial Cheating option 1: NERDA, Thanks to the lovely people at Ekstrabladet/PIN Cheating option 2: Simpletransformers Try custom Named Entities Skills, Scientific Terms Make your own NER dataset using Argilla and 0-shot labeling with Flair - you can deploy argilla with 1-click on ü§ó spaces and connect to it via e.</description></item></channel></rss>